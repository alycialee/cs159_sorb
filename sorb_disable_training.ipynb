{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sorb_disable_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b7a726cb17f452c839ecf3439d7b3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26e2aa4cf8e94385b04fcb1f5130a1ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6862f74fc7ac42f283ff6a6df786ab37",
              "IPY_MODEL_6537c285fd0443d68c24b0e2313e6ebc"
            ]
          }
        },
        "26e2aa4cf8e94385b04fcb1f5130a1ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6862f74fc7ac42f283ff6a6df786ab37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_12a75622548441789c89f36f3d5f524b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36b6f872e3884378b9554e5989b53a49"
          }
        },
        "6537c285fd0443d68c24b0e2313e6ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b878cfe1dcdb43afb1922a48691f87c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [00:04&lt;00:00, 21.61it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25277840064943c89e17f00e5b26e149"
          }
        },
        "12a75622548441789c89f36f3d5f524b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36b6f872e3884378b9554e5989b53a49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b878cfe1dcdb43afb1922a48691f87c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25277840064943c89e17f00e5b26e149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01ca2320a0b745f78ddcb12a14ae5f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_81b71949d5264390b484bb548f8fd7ac",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_447c38f1ec2946e28774d6f92f5c3369",
              "IPY_MODEL_9441b81dc5a94b39b8600e35392a591a"
            ]
          }
        },
        "81b71949d5264390b484bb548f8fd7ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "447c38f1ec2946e28774d6f92f5c3369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_94a3fd66e84f4b47ba049e0638407647",
            "_dom_classes": [],
            "description": "  7%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 30000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2148,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5026bea971444b3b9b88248aaef39d6f"
          }
        },
        "9441b81dc5a94b39b8600e35392a591a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ff8f32b07b447e686cb52a313762ca1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2148/30000 [00:42&lt;07:47, 59.61it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28c3654887af45348484dc16016ce9bc"
          }
        },
        "94a3fd66e84f4b47ba049e0638407647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5026bea971444b3b9b88248aaef39d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ff8f32b07b447e686cb52a313762ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28c3654887af45348484dc16016ce9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alycialee/cs159_sorb/blob/master/sorb_disable_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJhveYqWn-tA"
      },
      "source": [
        "##### Copyright 2019 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "pKe5zvMRoAB7",
        "colab": {}
      },
      "source": [
        "#@title License\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dc1snEt7qwn6"
      },
      "source": [
        "# [_Search on the Replay Buffer_: Bridging Planning and Reinforcement Learning](https://arxiv.org/abs/1906.05253)\n",
        "\n",
        "*Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine*\n",
        "\n",
        "\n",
        "What is *SoRB*? *SoRB* is a machine learning algorithm that learns to make decisions to reach goals. Typically, these sorts of control algorithms either rely on planning methods or reinforcement learning methods, both of which have limitations. Planning algorithms can reason over long horizons, but cannot deal with high-dimensional observations. Reinforcement learning algorithms fail to plan over long distances. *SoRB* attempts to combine the best of both algorithms.\n",
        "\n",
        "\n",
        "<center><img src=\"http://drive.google.com/uc?export=view&id=1LCqQJb06vyBOHl1uX-bi1p_LvuDMfPIF\" \n",
        "alt=\"search on the replay buffer\" width=\"800px\"/></center>\n",
        "The figure above highlights the basic idea.\n",
        "\n",
        "* (a) Goal-conditioned RL often fails to reach distant goals, but can successfully reach nearby goals (indicated in green).\n",
        "* (b) Our goal is to use observations in our replay buffer (yellow squares) as waypoints leading to the goal.\n",
        "* (c) We automatically find these waypoints by using the agent's value function to predict when two states are nearby, and building the corresponding graph.\n",
        "* (d) We run graph search to find the sequence of waypoints (blue arrows), and then use our goal-conditioned policy to reach each waypoint.\n",
        "\n",
        "<center><img src=\"http://drive.google.com/uc?export=view&id=1RsSVVYEcRADJmd78JYLARR0nGJtrpIN9\" \n",
        "alt=\"search on the replay buffer\" width=\"400px\"/></center>\n",
        "In our paper, we show how this algorithm can be used to solve complex visual navigation tasks, like the one shown above. In this colab notebook, we implement a basic version of *SoRB* on a simple navigation task. Interactive visualizations below allow you to explore the effect of various hyperparameters, as well as train your own agents.\n",
        "\n",
        "### Related Work\n",
        "A number of prior works have proposed methods for learning goal-conditioned policies and combining planning with RL. We encourage you to check out these related works:\n",
        "* Kaelbling, Leslie Pack. \"Learning to achieve goals.\" IJCAI. 1993.\n",
        "* Schaul, Tom, et al. \"Universal value function approximators.\" International conference on machine learning. 2015.\n",
        "* Pong, Vitchyr, et al. \"Temporal difference models: Model-free deep rl for model-based control.\" arXiv preprint arXiv:1802.09081 (2018).\n",
        "* Francis, Anthony, et al. \"Long-Range Indoor Navigation with PRM-RL.\" arXiv preprint arXiv:1902.09458 (2019).\n",
        "* Savinov, Nikolay, Alexey Dosovitskiy, and Vladlen Koltun. \"Semi-parametric topological memory for navigation.\" arXiv preprint arXiv:1803.00653 (2018).\n",
        "\n",
        "\n",
        "If you find this code useful, please consider citing our paper: [https://arxiv.org/abs/1906.05253](https://arxiv.org/abs/1906.05253)\n",
        "```\n",
        "@misc{eysenbach2019,\n",
        "Author = {Benjamin Eysenbach and Ruslan Salakhutdinov and Sergey Levine},\n",
        "Title = {Search on the Replay Buffer: Bridging Planning and Reinforcement Learning},\n",
        "Year = {2019},\n",
        "Eprint = {arXiv:1906.05253},\n",
        "}\n",
        "```\n",
        "\n",
        "### Getting Started\n",
        "To get started, click the \"Connect\" button in the top right corner of the screen. You should see a green checkmark next to some stats about RAM and Disk. Run each of the cells below, either by clicking the \"run cell\" button on the left of each cell, or by pressing [ctrl]+[enter]. **Double click on any cell to see the code.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "JpFNoJBc7zur",
        "outputId": "b6d52d31-2f09-492d-99f1-9b606860d101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "#@title Install Dependencies\n",
        "!pip install tensorflow\n",
        "!pip install tf-agents"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.28.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0rc0)\n",
            "Requirement already satisfied: gin-config==0.1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.1.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.18.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.8.0->tf-agents) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.8.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.8.0->tf-agents) (0.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "bbiO8OnH8Bk7",
        "colab": {}
      },
      "source": [
        "#@title Import dependencies.\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "import gym\n",
        "import gym.spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy.sparse.csgraph\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.agents.ddpg import actor_network\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import gym_wrapper\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import utils\n",
        "from tf_agents.policies import actor_policy\n",
        "from tf_agents.policies import ou_noise_policy\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import time_step\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "# tf.enable_eager_execution()\n",
        "# tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "esd_jQgISaff",
        "outputId": "42dd8350-434a-4e52-be07-e35ad4c845bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "source": [
        "#@title Implement the 2D navigation environment and helper functions.\n",
        "WALLS = {\n",
        "    'Small':\n",
        "        np.array([[0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0]]),\n",
        "    'Cross':\n",
        "        np.array([[0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 1, 0, 0, 0],\n",
        "                  [0, 0, 0, 1, 0, 0, 0],\n",
        "                  [0, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 0, 0, 1, 0, 0, 0],\n",
        "                  [0, 0, 0, 1, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 0]]),\n",
        "    'FourRooms':\n",
        "        np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                  [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
        "                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]),\n",
        "    'Spiral5x5':\n",
        "        np.array([[0, 0, 0, 0, 0],\n",
        "                  [0, 1, 1, 1, 1],\n",
        "                  [0, 1, 0, 0, 1],\n",
        "                  [0, 1, 1, 0, 1],\n",
        "                  [0, 0, 0, 0, 1]]),\n",
        "    'Spiral7x7':\n",
        "        np.array([[1, 1, 1, 1, 1, 1, 1],\n",
        "                  [1, 0, 0, 0, 0, 0, 0],\n",
        "                  [1, 0, 1, 1, 1, 1, 0],\n",
        "                  [1, 0, 1, 0, 0, 1, 0],\n",
        "                  [1, 0, 1, 1, 0, 1, 0],\n",
        "                  [1, 0, 0, 0, 0, 1, 0],\n",
        "                  [1, 1, 1, 1, 1, 1, 0]]),\n",
        "    'Spiral9x9':\n",
        "        np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                  [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
        "                  [0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
        "                  [0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
        "                  [0, 1, 0, 1, 1, 0, 1, 0, 1],\n",
        "                  [0, 1, 0, 0, 0, 0, 1, 0, 1],\n",
        "                  [0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
        "                  [0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
        "    'Spiral11x11':\n",
        "        np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "                  [1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
        "                  [1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
        "                  [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0],\n",
        "                  [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
        "                  [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
        "                  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]),\n",
        "    'Maze3x3':\n",
        "        np.array([[0, 0, 0],\n",
        "                  [1, 1, 0],\n",
        "                  [0, 0, 0]]),\n",
        "    'Maze6x6':\n",
        "        np.array([[0, 0, 1, 0, 0, 0],\n",
        "                  [1, 0, 1, 0, 1, 0],\n",
        "                  [0, 0, 1, 0, 1, 1],\n",
        "                  [0, 1, 1, 0, 0, 1],\n",
        "                  [0, 0, 1, 1, 0, 1],\n",
        "                  [1, 0, 0, 0, 0, 1]]),\n",
        "    'Maze11x11':\n",
        "        np.array([[0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "                  [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "                  [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
        "                  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
        "                  [1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0],\n",
        "                  [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0],\n",
        "                  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
        "                  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
        "    'Tunnel':\n",
        "        np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
        "                  [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
        "                  [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "                  [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
        "                  [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
        "                  [0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
        "                  [0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
        "                  [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
        "                  [0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
        "                  [0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                  [0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                  [0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
        "                  [0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0],\n",
        "                  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]),\n",
        "    'U':\n",
        "        np.array([[0, 0, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [1, 1, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [0, 0, 0]]),\n",
        "    'Tree':\n",
        "        np.array([\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
        "            [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0],\n",
        "            [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "        ]),\n",
        "    'UMulti':\n",
        "        np.array([\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "         ]),\n",
        "    'FlyTrapSmall':\n",
        "        np.array([\n",
        "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
        "         ]),\n",
        "    'FlyTrapBig':\n",
        "        np.array([\n",
        "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
        "         ]),\n",
        "    'Galton':\n",
        "        np.array([\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
        "            [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "            [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "        ]),\n",
        "}\n",
        "\n",
        "def resize_walls(walls, factor):\n",
        "  \"\"\"Increase the environment by rescaling.\n",
        "\n",
        "  Args:\n",
        "    walls: 0/1 array indicating obstacle locations.\n",
        "    factor: (int) factor by which to rescale the environment.\"\"\"\n",
        "  (height, width) = walls.shape\n",
        "  row_indices = np.array([i for i in range(height) for _ in range(factor)])\n",
        "  col_indices = np.array([i for i in range(width) for _ in range(factor)])\n",
        "  walls = walls[row_indices]\n",
        "  walls = walls[:, col_indices]\n",
        "  assert walls.shape == (factor * height, factor * width)\n",
        "  return walls\n",
        "\n",
        "\n",
        "\n",
        "class PointEnv(gym.Env):\n",
        "  \"\"\"Abstract class for 2D navigation environments.\"\"\"\n",
        "\n",
        "  def __init__(self, walls=None, resize_factor=1,\n",
        "               action_noise=1.0):\n",
        "    \"\"\"Initialize the point environment.\n",
        "\n",
        "    Args:\n",
        "      walls: (str) name of one of the maps defined above.\n",
        "      resize_factor: (int) Scale the map by this factor.\n",
        "      action_noise: (float) Standard deviation of noise to add to actions. Use 0\n",
        "        to add no noise.\n",
        "    \"\"\"\n",
        "    if resize_factor > 1:\n",
        "      self._walls = resize_walls(WALLS[walls], resize_factor)\n",
        "    else:\n",
        "      self._walls = WALLS[walls]\n",
        "    self._apsp = self._compute_apsp(self._walls)\n",
        "    (height, width) = self._walls.shape\n",
        "    self._height = height\n",
        "    self._width = width\n",
        "    self._action_noise = action_noise\n",
        "    self.action_space = gym.spaces.Box(\n",
        "        low=np.array([-1.0, -1.0]),\n",
        "        high=np.array([1.0, 1.0]),\n",
        "        dtype=np.float32)\n",
        "    self.observation_space = gym.spaces.Box(\n",
        "        low=np.array([0.0, 0.0]),\n",
        "        high=np.array([self._height, self._width]),\n",
        "        dtype=np.float32)\n",
        "    self.reset()\n",
        "\n",
        "  def _sample_empty_state(self):\n",
        "    candidate_states = np.where(self._walls == 0)\n",
        "    num_candidate_states = len(candidate_states[0])\n",
        "    state_index = np.random.choice(num_candidate_states)\n",
        "    state = np.array([candidate_states[0][state_index],\n",
        "                      candidate_states[1][state_index]],\n",
        "                     dtype=np.float)\n",
        "    state += np.random.uniform(size=2)\n",
        "    assert not self._is_blocked(state)\n",
        "    return state\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = self._sample_empty_state()\n",
        "    return self.state.copy()\n",
        "\n",
        "  def _get_distance(self, obs, goal):\n",
        "    \"\"\"Compute the shortest path distance.\n",
        "\n",
        "    Note: This distance is *not* used for training.\"\"\"\n",
        "    (i1, j1) = self._discretize_state(obs)\n",
        "    (i2, j2) = self._discretize_state(goal)\n",
        "    return self._apsp[i1, j1, i2, j2]\n",
        "\n",
        "  def _discretize_state(self, state, resolution=1.0):\n",
        "    (i, j) = np.floor(resolution * state).astype(np.int)\n",
        "    # Round down to the nearest cell if at the boundary.\n",
        "    if i == self._height:\n",
        "      i -= 1\n",
        "    if j == self._width:\n",
        "      j -= 1\n",
        "    return (i, j)\n",
        "\n",
        "  def _is_blocked(self, state):\n",
        "    if not self.observation_space.contains(state):\n",
        "      return True\n",
        "    (i, j) = self._discretize_state(state)\n",
        "    return (self._walls[i, j] == 1)\n",
        "\n",
        "  def step(self, action):\n",
        "    if self._action_noise > 0:\n",
        "      action += np.random.normal(0, self._action_noise)\n",
        "    action = np.clip(action, self.action_space.low, self.action_space.high)\n",
        "    assert self.action_space.contains(action)\n",
        "    num_substeps = 10\n",
        "    dt = 1.0 / num_substeps\n",
        "    num_axis = len(action)\n",
        "    for _ in np.linspace(0, 1, num_substeps):\n",
        "      for axis in range(num_axis):\n",
        "        new_state = self.state.copy()\n",
        "        new_state[axis] += dt * action[axis]\n",
        "        if not self._is_blocked(new_state):\n",
        "          self.state = new_state\n",
        "\n",
        "    done = False\n",
        "    rew = -1.0 * np.linalg.norm(self.state)\n",
        "    return self.state.copy(), rew, done, {}\n",
        "\n",
        "  @property\n",
        "  def walls(self):\n",
        "    return self._walls\n",
        "\n",
        "  def _compute_apsp(self, walls):\n",
        "    (height, width) = walls.shape\n",
        "    g = nx.Graph()\n",
        "    # Add all the nodes\n",
        "    for i in range(height):\n",
        "      for j in range(width):\n",
        "        if walls[i, j] == 0:\n",
        "          g.add_node((i, j))\n",
        "\n",
        "    # Add all the edges\n",
        "    for i in range(height):\n",
        "      for j in range(width):\n",
        "        for di in [-1, 0, 1]:\n",
        "          for dj in [-1, 0, 1]:\n",
        "            if di == dj == 0: continue  # Don't add self loops\n",
        "            if i + di < 0 or i + di > height - 1: continue  # No cell here\n",
        "            if j + dj < 0 or j + dj > width - 1: continue  # No cell here\n",
        "            if walls[i, j] == 1: continue  # Don't add edges to walls\n",
        "            if walls[i + di, j + dj] == 1: continue  # Don't add edges to walls\n",
        "            g.add_edge((i, j), (i + di, j + dj))\n",
        "\n",
        "    # dist[i, j, k, l] is path from (i, j) -> (k, l)\n",
        "    dist = np.full((height, width, height, width), np.float('inf'))\n",
        "    for ((i1, j1), dist_dict) in nx.shortest_path_length(g):\n",
        "      for ((i2, j2), d) in dist_dict.items():\n",
        "        dist[i1, j1, i2, j2] = d\n",
        "    return dist\n",
        "\n",
        "class GoalConditionedPointWrapper(gym.Wrapper):\n",
        "  \"\"\"Wrapper that appends goal to state produced by environment.\"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, env, prob_constraint=0.8, min_dist=0, max_dist=4,\n",
        "               threshold_distance=1.0):\n",
        "    \"\"\"Initialize the environment.\n",
        "\n",
        "    Args:\n",
        "      env: an environment.\n",
        "      prob_constraint: (float) Probability that the distance constraint is\n",
        "        followed after resetting.\n",
        "      min_dist: (float) When the constraint is enforced, ensure the goal is at\n",
        "        least this far from the initial state.\n",
        "      max_dist: (float) When the constraint is enforced, ensure the goal is at\n",
        "        most this far from the initial state.\n",
        "      threshold_distance: (float) States are considered equivalent if they are\n",
        "        at most this far away from one another.\n",
        "    \"\"\"\n",
        "    self._threshold_distance = threshold_distance\n",
        "    self._prob_constraint = prob_constraint\n",
        "    self._min_dist = min_dist\n",
        "    self._max_dist = max_dist\n",
        "    super(GoalConditionedPointWrapper, self).__init__(env)\n",
        "    self.observation_space = gym.spaces.Dict({\n",
        "        'observation': env.observation_space,\n",
        "        'goal': env.observation_space,\n",
        "    })\n",
        "\n",
        "  def _normalize_obs(self, obs):\n",
        "    return np.array([\n",
        "        obs[0] / float(self.env._height),\n",
        "        obs[1] / float(self.env._width)\n",
        "    ])\n",
        "\n",
        "  def reset(self):\n",
        "    goal = None\n",
        "    count = 0\n",
        "    while goal is None:\n",
        "      obs = self.env.reset()\n",
        "      (obs, goal) = self._sample_goal(obs)\n",
        "      count += 1\n",
        "      if count > 1000:\n",
        "        print('WARNING: Unable to find goal within constraints.')\n",
        "    self._goal = goal\n",
        "    return {'observation': self._normalize_obs(obs),\n",
        "            'goal': self._normalize_obs(self._goal)}\n",
        "\n",
        "  def step(self, action):\n",
        "    obs, _, _, _ = self.env.step(action)\n",
        "    rew = -1.0\n",
        "    done = self._is_done(obs, self._goal)\n",
        "    return {'observation': self._normalize_obs(obs),\n",
        "            'goal': self._normalize_obs(self._goal)}, rew, done, {}\n",
        "\n",
        "  def set_sample_goal_args(self, prob_constraint=None,\n",
        "                           min_dist=None, max_dist=None):\n",
        "    assert prob_constraint is not None\n",
        "    assert min_dist is not None\n",
        "    assert max_dist is not None\n",
        "    assert min_dist >= 0\n",
        "    assert max_dist >= min_dist\n",
        "    self._prob_constraint = prob_constraint\n",
        "    self._min_dist = min_dist\n",
        "    self._max_dist = max_dist\n",
        "\n",
        "  def _is_done(self, obs, goal):\n",
        "    \"\"\"Determines whether observation equals goal.\"\"\"\n",
        "    return np.linalg.norm(obs - goal) < self._threshold_distance\n",
        "\n",
        "  def _sample_goal(self, obs):\n",
        "    \"\"\"Sampled a goal state.\"\"\"\n",
        "    if np.random.random() < self._prob_constraint:\n",
        "      return self._sample_goal_constrained(obs, self._min_dist, self._max_dist)\n",
        "    else:\n",
        "      return self._sample_goal_unconstrained(obs)\n",
        "\n",
        "  def _sample_goal_constrained(self, obs, min_dist, max_dist):\n",
        "    \"\"\"Samples a goal with dist min_dist <= d(obs, goal) <= max_dist.\n",
        "\n",
        "    Args:\n",
        "      obs: observation (without goal).\n",
        "      min_dist: (int) minimum distance to goal.\n",
        "      max_dist: (int) maximum distance to goal.\n",
        "    Returns:\n",
        "      obs: observation (without goal).\n",
        "      goal: a goal state.\n",
        "    \"\"\"\n",
        "    (i, j) = self.env._discretize_state(obs)\n",
        "    mask = np.logical_and(self.env._apsp[i, j] >= min_dist,\n",
        "                          self.env._apsp[i, j] <= max_dist)\n",
        "    mask = np.logical_and(mask, self.env._walls == 0)\n",
        "    candidate_states = np.where(mask)\n",
        "    num_candidate_states = len(candidate_states[0])\n",
        "    if num_candidate_states == 0:\n",
        "      return (obs, None)\n",
        "    goal_index = np.random.choice(num_candidate_states)\n",
        "    goal = np.array([candidate_states[0][goal_index],\n",
        "                     candidate_states[1][goal_index]],\n",
        "                    dtype=np.float)\n",
        "    goal += np.random.uniform(size=2)\n",
        "    dist_to_goal = self.env._get_distance(obs, goal)\n",
        "    assert min_dist <= dist_to_goal <= max_dist\n",
        "    assert not self.env._is_blocked(goal)\n",
        "    return (obs, goal)\n",
        "\n",
        "  def _sample_goal_unconstrained(self, obs):\n",
        "    \"\"\"Samples a goal without any constraints.\n",
        "\n",
        "    Args:\n",
        "      obs: observation (without goal).\n",
        "    Returns:\n",
        "      obs: observation (without goal).\n",
        "      goal: a goal state.\n",
        "    \"\"\"\n",
        "    return (obs, self.env._sample_empty_state())\n",
        "\n",
        "  @property\n",
        "  def max_goal_dist(self):\n",
        "    apsp = self.env._apsp\n",
        "    return np.max(apsp[np.isfinite(apsp)])\n",
        "\n",
        "\n",
        "class NonTerminatingTimeLimit(wrappers.PyEnvironmentBaseWrapper):\n",
        "  \"\"\"Resets the environment without setting done = True.\n",
        "\n",
        "  Resets the environment if either these conditions holds:\n",
        "    1. The base environment returns done = True\n",
        "    2. The time limit is exceeded.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, env, duration):\n",
        "    super(NonTerminatingTimeLimit, self).__init__(env)\n",
        "    self._duration = duration\n",
        "    self._step_count = None\n",
        "\n",
        "  def _reset(self):\n",
        "    self._step_count = 0\n",
        "    return self._env.reset()\n",
        "\n",
        "  @property\n",
        "  def duration(self):\n",
        "    return self._duration\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._step_count is None:\n",
        "      return self.reset()\n",
        "\n",
        "    ts = self._env.step(action)\n",
        "\n",
        "    self._step_count += 1\n",
        "    if self._step_count >= self._duration or ts.is_last():\n",
        "      self._step_count = None\n",
        "\n",
        "    return ts\n",
        "\n",
        "def env_load_fn(environment_name,\n",
        "         max_episode_steps=None,\n",
        "         resize_factor=1,\n",
        "         gym_env_wrappers=(GoalConditionedPointWrapper,),\n",
        "         terminate_on_timeout=False):\n",
        "  \"\"\"Loads the selected environment and wraps it with the specified wrappers.\n",
        "\n",
        "  Args:\n",
        "    environment_name: Name for the environment to load.\n",
        "    max_episode_steps: If None the max_episode_steps will be set to the default\n",
        "      step limit defined in the environment's spec. No limit is applied if set\n",
        "      to 0 or if there is no timestep_limit set in the environment's spec.\n",
        "    gym_env_wrappers: Iterable with references to wrapper classes to use\n",
        "      directly on the gym environment.\n",
        "    terminate_on_timeout: Whether to set done = True when the max episode\n",
        "      steps is reached.\n",
        "\n",
        "  Returns:\n",
        "    A PyEnvironmentBase instance.\n",
        "  \"\"\"\n",
        "  gym_env = PointEnv(walls=environment_name,\n",
        "                     resize_factor=resize_factor)\n",
        "\n",
        "  for wrapper in gym_env_wrappers:\n",
        "    gym_env = wrapper(gym_env)\n",
        "  env = gym_wrapper.GymWrapper(\n",
        "      gym_env,\n",
        "      discount=1.0,\n",
        "      auto_reset=True,\n",
        "  )\n",
        "\n",
        "  if max_episode_steps > 0:\n",
        "    if terminate_on_timeout:\n",
        "      env = wrappers.TimeLimit(env, max_episode_steps)\n",
        "    else:\n",
        "      env = NonTerminatingTimeLimit(env, max_episode_steps)\n",
        "\n",
        "  return tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "def plot_walls(walls):\n",
        "  walls = walls.T\n",
        "  (height, width) = walls.shape\n",
        "  for (i, j) in zip(*np.where(walls)):\n",
        "    x = np.array([j, j+1]) / float(width)\n",
        "    y0 = np.array([i, i]) / float(height)\n",
        "    y1 = np.array([i+1, i+1]) / float(height)\n",
        "    plt.fill_between(x, y0, y1, color='grey')\n",
        "  plt.xlim([0, 1])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "for index, (name, walls) in enumerate(WALLS.items()):\n",
        "  plt.subplot(3, 6, index + 1)\n",
        "  plt.title(name)\n",
        "  plot_walls(walls)\n",
        "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
        "plt.suptitle('Navigation Environments', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAHACAYAAACMK4GPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxkVX3///d7gAbZZsBBW9ZRSNyjmChqMMFEo6h8IUbjAsqYYECDfv2pieI2PSqMIYmCqIH4NQ4CLgiIu+ISVDCo4ChqXBAYGMBWFmdYBhigP78/zmm9U1PVXd1dyz23Xs/Hox7dVXc79566pz733HPOdUQIAAAAKMGiYScAAAAA6BbBKwAAAIpB8AoAAIBiELwCAACgGASvAAAAKAbBKwAAAIpB8Ao0nO21ttcOOx2tbC+3HbaXDzstdWb7oHycJoadFgCoA4JXoAdycBG2r7G9XYd51uZ5th50+oahxKCrElDP9Fo77HRifmwvy3m4ethpATB/I/EjCgzQ3pJeI+ldw05IxV8OOwEdfErSJZJ+NeyEtPFDSed3mLZ+kAmR9F1JD5d004C3CwC1RPAK9M5vJYWkN9r+fxFRi2AjIq4cdhraiYgNkjYMOx0d/CAiJoadCEmKiI2SfjbsdABAXdBsAOidjZLeIWmxpBXdLpRvVZ9r+yrbd9q+1fbFto9oM+/PbG+yvbTDut6Qb4seW/msbZtX24ttn2T7Ott35XW/1vZD2t1atf2Htt9l+1LbN9q+OzeT+E/be7bMu1rSf+e3K1puux9U2e+2bV5t/3E+Jr+pbOcDth/UZt7VeT3LbB9t+0d5f36d07a43bHqlenja3sH2/9q+9qc5l/m/HBl3ifmtH5qhvX9NC+/a37ftvmF7Qvz52O232b753m51ZV5+nocK/u+o+332F6Xv8M/sH1Ynmdr22+2fUVe35XV72ebdT7D9hds35TTfGU+rksWeOwnJF2d3x7Z8p1cnuex7SNtfzt/x+/K+/Rl2y/olGYAg0XNK9Bb75d0rKSjbb83Iq7oYpn/kPQTSd9UuoV+f0nPknSG7YdGxFsr854u6QRJL5J0Spt1HSlpk6SPzrRBp3a5X5f0OElrJJ2lFHS/WdJTOiz2XEnHKAWl387beaSkoyQdYvtPIuL6PO/0LfcjJX1D0oWV9aydJW3PkXSuJEs6R9I1kv5Y0iskHWr7wIi4us2iJ0p6hqTPSrpA0lMlvVzSfpL+YqZt9sA2kr4saXdJX5R0r6TDlJqPbCdppSRFxCW2fy7pWbbvHxE3V1di+wmSHibp3Ii4pcttnyvp8Xm750v6TV7XoI7jNpK+ImlXSZ+WNKb0/TzX9l9JeqWkA3L67pb0fEmn2L4xIj7Rsv8rJE1IukXS5/K+/JGk1ysdsydFxK1ttj/rsVf6Di6R9H+1ZbOQH+S/x0s6TinIPVvpzsCDlI7v8yVtll4AQxIRvHjxWuBLqbnAdfn/5+X357XMszZ/vnXL5/u2Wd+YpK9JukfSHpXP95R0n6RL2yzz+Lz+c9tsd23LZ2/N835Mkiuf7yXpxjxtdcsye0jats12/yqn6T9aPj8or2eiwzFbnqcvr3y2o6Sb8/qe0jL/G/L8F7R8vjp/fq2kvSufb610QRCSntBlPk6n6QdKQVS71zM75OsXJN2v8vkDlNrHrpe0TeXz4/L8x7bZ/vvztENmO45KwVhIulzS0pZpAzmOlX3/bPW7oXQBFEpB6PckLalMe4jShc+alnU9NS/z7er8LfnyngUe+2Vq892uTL9Z0nWStm8zbWm7ZXjx4jX4F80GgB6LiHMk/Y+kv7Z9YBfzb9EmNSI2KQUyW6vS4SoirlMKav/Y9iNbFjsy/z29i2QeKWlK0nEREZX1r5N0Uod0Xh8Rd7f5/AKlmuNndLHd2RyqVIP3iYj4Vsu0f1cKVp5ue+82y749Iq6tpOteSR/Ob58wx3Q8RqnpR7vXMzss8+qIuLOy/d8o1UQulvTQynxnKB37I6sL2x6T9EKl2sYvziGtb40t21cP+ji+pvrdyNu8WtIukt4QEesr066SdLGkR9neqrKOV+e/L6/On5dZrXRBcXiH7Xd77Ltxj1LQv5k2xxjAkBC8Av3xuvz332ab0fbett/v1OZ043Q7PKVbvlKq8axanf8eWVnH9K3a3yjVQs20vZ0l7Svp+ohY22aWizosZ9tH2P5qbg94byWtj26Tzvl4XP779dYJOYj6Zn67f5tlL23z2br8d5c5puP0iHCH12vazL8hIn7ZzfYrFyB/YvsRlXkPUQo4z8r72q3vtvlskMdxfbsLMEk35L+XtZl2vdKF2XjlsycpBY7Ptz3R+lK6G7Gb7fu3rKvrY9+Fs5RqZ//X9irbz2zX1hfAcNHmFeiDiPgf2+dIep7tF0RL275pth+iFHzsIulbSm0MNyjV/CxTClC3bVnsU5JulXSE7eMi4j5Jz1EKfE7qIvDZOf/9dYfpnT5/t9IwYL9SamN4vaTp2q7lkvaZZbvdmA4UOg2fNf35Fp131H4Iq+ljsVWbab3UafisTttfLenpSvn7hvzZXGrOqybbfDbI49hpxIh7pd+NKtFpfdtUPru/0m/SbJ0dp5tETJvrsZ/J/yfpKkkvk/TG/LrX9hckva5DkAxgwAhegf45Tun27Sp37l3+WqUf7ZflW6O/Y/tFarm1LEkRcafts5U6Sj1d0pc0t8BnusPLAztM3+Jz2w9Quq37Y0lPjojb2qS1F6YDnfEO0x/UMl+pqhcgb1L6Dhws6YcR8cO5rKja7KOixOO4QdKiiNh1WAnIF4InSTopf+cPVGrK8XxJj7T9yHZNZwAMFs0GgD7JtTQfkPRgSa/qMNt++e+5bab9+QyrX53/Hml7N6XA5/KI+EHnRX6XrluVapf2sL2szSzt2uk+RKm8uKBN4Lpnnt5qut3gXGq+1uS/B7VOcHoy2fRICN+fwzprJ7fPPFuph/zTJL1YqTJhrrWunZR4HC+RtEubtty91PV3MiJ+ExHnRcTfKjW/2FfSo/qYNgBdIngF+uvtSrc136x0u7PV2vz3oOqHtp+hVLPaVkRcLOkKpZrdY5Ruv66eQ7o+onT+r2oZC3MvpaYBndJ5YLWTje0dJX1Q7e/iTN/abdcpqJPzlXqov8j2E1umvUbpQuCr1Q5FBVud/740v+5VanPZCyUex/fkvx+0vXvrxDyWa+u+zNX0g0S2+E7a3tb2n7b5fBulJjlSGssZwJDRbADoo4i4xfYJSmNntvMBpfZ1n8xtZG9Qqt15plLN3EwDo39E6aEIb9XcA58TlcbCfKGkh9q+QKmd5N8qdeY5TKlH/PR+TNr+eJ7/B5X5ny7pLqWe4I9t2cbPldrFvtD2PUrjjIakMyLimnaJiojbbf+dpE9K+obtTyoN3fTHSkNyTUo6eg77OV+PdctDAVrS2XFatyLiYtu/VLolvY2kz+Ze8gtWo+PYtYj4mu03Slol6YrczvRqpYu+fZTuRFykzqM9dLON221/R9JTbJ8l6RdKtbGfUTo+F+U8uUzp+7qd0nf84ZI+ExE/ne+2AfQOwSvQf+9VGqh9WeuEiLjc9lMlvVPSs5XOyR8qPRBgvWYPXlcqBT6fm0vgk9vNPlWpZvh5Sh1VrlZ6AMK3lILX1sHg/16pucELJP2j0niwn5H0NrVp9hAR99n+a6XB4p8vaSelAfMvUgoMOqXt07kG7E1Kw28tVgq2TpX0joi4odOyPfSY/OpkokfbOV3pAmT6/56pyXGck4j4F9sXK7WvPlDpzsIGpYug/9QsD9/o0kuUanmfqTRCh5XGdv2pUue5p0p6stI5cJukK5Ue7PBfPdg2gB5w+7b+AEaV7ZcrBQrHRMRpw04PAABVBK/AiLK9e2vtWx60/iKl3uj71LF2DgAw2mg2AIyuc3NnlMuUmigsUxovdnulJ28RuAIAaoeaV2BE2X6lUvu/P1BqD3m70hBL74uI84aZNgAAOiF4BQAAQDEY5xUAAADFIHgFAABAMQheAQAAUAyCVwAAABSD4BUAAADFIHgFAABAMQheAQAAUAyCVwAAABSD4BUAAADFIHgFAABAMQheAQAAUAyCVwAAABSD4BUAAADFIHgFAABAMQheAQAAUAyC11nYXmY7bG+d319o+6hhpwsA+sX2F20fuYDl19p+Wi/ThLkhD8tHHnZWdPBq+0Db37a9wfYtti+2/fhhpwtzZ/vFti+1fbvtX+WT9sBhp2sU5ALuznzsp1+793gbq21vyuu+xfZXbD+sl9vAluZbRkbEwRFxeo/SsNz2fS3fr4O6XPZC23dVlvt5L9JUkprk4akt+Xe37du6XPYnLcvea/uzvUhXKWqSh9vafo/tG2z/1vYHbG/T5bJ72P50Tvt1to/pRZoWotjg1fbOkj4n6RRJu0raQ9JKSXcPM12YO9uvlXSSpBMkPVDS3pI+IOnQNvNuPdjUjYxDImLHyuuGXq3Y9lb53xMjYkelc/V6SR/q1TawpX6VkfM8B/+n5ft14RyWPbay3EPnse1i1SUPI+KYav5J+pikT3a57CMry+0kaV23yzZBXfJQ0hsl/YmkR0n6Q0mPk/SWLpc9U9LVSr/Pz5Z0gu2nznH7PVVs8Kp08BURH4uI+yLizoi4ICIuz1f6F+erjPW2r7L95Pz5Otu/qVbF23627TW2b83TJ4a2VyPG9mJJb5f0jxFxXkTcERH3RMRnI+KfbE/YPsf2mbZvlbTc9u62P5OvAn9p++WV9T0h1+DeavvXtt+dP98ur+Pm/J34nu0HDmm3ay9fpZ+Ur9JvyP9vm6ctt31Ry/xhe7/8/2rb/2H7C7bvkLRZIRcRd0o6W9JjK8s/PNeyrc81Nf+nMm2x7Y/YvtH2NbbfYntRJS1zOdefZft/bd9m+3rbr+/D4auLbsrI9+XaoJ/Z/svpBV1pHtVyjG+WNGF7X9tfz+fTTbbPsr1krgnMeXWT7b3y+8fkWiFq5ZPa5aHtHST9jaTT8/t9c1n8uPx+93yuHtRm8T+TtFTSuQs8LiWpSx4eIum9EXFLRNwo6b2S/i6vu2Me2t5R0kGSjs+/zT+UdM70ssNScvD6C0n32T7d9sG2d2mZfoCkyyXdX9JHJX1c0uMl7SfpCEnvy5kiSXdIeqmkJUpXFa+wfdgA9gHSkyRtJ+lTM8xzqNLJskTSWUp5eZ2k3SU9T+kq8C/yvCdLOjkidpa0r1KQJElHSlosaS+l78Qxku7s6Z40y5slPVEpwHyMpCeo+6t0SXqxpOOValpaA90dJL1I0i/z+20kfVbSBZIeIOlVks6yPV3LdopS3j1E0p8rnasvq6xyLuf6hyQdHRE7KdVAfH0O+1SabsrIK5WCiRWSzrO9a4d1HSDpKqWal+MlWdIqpXPw4Urn1cQMadk//7j+wvZbnWuNIuLbkk6TdLrt+ynV8Lw1In5WWXZVXvbiDgFRk9UpD6f9jaQbJX1TkiLiSklvkHSm7e0lfVjS6R1q14+UdG5E3NHFdpqiTnnolv/3tL14ljx0h2UfNcN2+i8iin0pZdZqpUDmXkmfUcrU5ZKuqMz3aEkh6YGVz26W9NgO6z1J0nvy/8vyslvn9xdKOmrY+96Ul6TDJU3OMH1C0jcr7/eSdJ+knSqfrZK0Ov//TaVbMktb1vN3kr4t6Y+Gvc91e0laK+l2Sevz63ylwvRZlXmeIWlt/n+5pIta1hGS9sv/r5b0kZbpqyXdldc/pXQL6o/ytKdImpS0qDL/x3LebyVpk6RHVKYdLenCSlq6PtclXZuX33nYx31AeTtTGXmDJFfm/a6kl+T/f1fO5XmvnWU7h0la0/Kdelr+/yGSHqxUWfJoSf8r6bjKvNtIukzSjyR9qSVNByhdAG2rFPjcJmnfYR/XUcvDlvm+JmmizeefyXl4uaRt20zfXtKtkg4a9jEdxTyU9E5JF0vaTdK4pO8olZUPmi0PlSogTlGqaHqcpFsk/XyYx7TkmldFxE8jYnlE7Kl0FbC7UuApSb+uzHpnnr/1sx0lyfYBtv87V5NvUKqVW9r3HYCUAoulnrn9zrrK/7tLuiUiqp0FrlFqRyRJf690m+ZnTk0DnpM/P0PSlyV93Ok2+InusrH6iDgsIpbk12FKx/mayvRr8mfdWtfms3+LiCVKF4R3SpquWd1d0rqImGrZ3h5K5+E2bdKyR+V91+e6Uq3RsyRdY/sbtp80h30qzixl5PWRf5mymfJ4s/y0/UDbH3dqenGrUo1p2zIzIq6KiKsjYioifqTUTOh5len3KP2wP0rSv1fTFBHfiYjbIuLuSB1XLlbKv5FRhzysLLO30i3kj7SZ/MGcvlMiol17zucqBT3fmGkbTVSTPDxe0hpJP1CqyDlf0j3avPzslIeHK12ArpP0H3k7182wy31XdPBaFek202rNryr7o0pXHHtFxGJJp2rzKnL0z/8oNVyfqZlG9cS+QdKutneqfLa3UgcgRcQVEfEipdvP/yLpHNs7RGqrszIiHiHpyZKeo3T7Ge3dIGmfyvu982dSamaz/fQE2+Ntlo82n6UJEddK+r+STs63im+QtJdzO9bK9q6XdJNSAdualuu73pPNt/29iDhU6ftxvn7frKTx2pSRe9iulnPVPN5i8Zb3J+TPHh2pic4R6r7MjOq8tvdQul36YUn/7ty2uptlR00N8vAlki6OiKuqH+ZmOScpNcuZ6HDb+0ilOzIdy4ZRMKw8jNTW9tiI2CMiHqJUcXTZdKXBTHkYEddExHMiYreIOEApQP5u1zvdB8UGr7YfZvt1tvfM7/dSakd3yTxWt5NSbd5dtp+g1F4PAxARGyS9TdL7bR9me3vb2+S2QSe2mX+d0lXjKqdOWH+kVNt6piTZPsL2bvmEXJ8Xm7L9VNuPdur5fqtSQDTVun78zsckvcX2braXKuXRmXnaDyU90vZjbW+n7trJbSYivqJUQP+D0u2rjZL+Oef9QUqdCz4eEfcpBZjH297J9j6SXltJS9dsj9k+PLfxukfpe9DY70AXZeQDJL06H/PnK93a/EKXq99JqanJhhx8/tMM6TjYuXOkU0est0r6dH5vpR/yDymdx7+S9I48bYntZ+TzfGvbhyt1+PlSt8egdHXJw4qXKuVXq5MlXRoRR0n6vFIFUHU/9lTquNmTYZ9KUpc8dBruancnT1Q6D1dUZumYh04danfKZegRkv5K0ru7TGNfFBu8KrV9OkDSd5x6NF8i6ceSXjePdb1S0tudxq17m0aoNqYOIuLflQKStyh1BFgn6VilmrF2XqR06/kGpY5eKyLiq3naMyX9xPbtSifjCyP1bh9X6vR1q6SfKt26OqMf+9MQ75R0qVLbpx9J+n7+TBHxC6Vbv1+VdIVaOmTNwb9K+melmoJDJB2sVNP6AUkvjd932nmVUm3vVXlbH5X0X/Pc5kskrc232I5Ruh3WVLOVkd+R9AdKx/x4Sc+LiJu7XPdKpbZvG5R+6M6bYd6/lHR5TsMX8rwn5GmvVvrxfmuukXuZpJfZfopSc5F3KpUJNyl9Dw7L379RUZc8VG5is6dahrmyfahSufuK/NFrJT0uX2xMe4nScGlXdpm2JqlLHu6rVPFzh9JFxBsj4gKpqzx8hlL5+1ulcvOZkUYsGBqPeA0+AIwc28uVOoLwIJBCkYflIw/nr+SaVwAAAIwYglcAAAAUg2YDAAAAKAY1rwAAACjGTAPDb2Hp0qWxbNmyPiUFC3HZZZfdFBG7zTYfeVhf5OHvTU5Oampq5lGsFi1apPHxdkPMDk+3eSj1Ph+7OWbozvr162Pjxo2zVu7ssMMOsXjx4nlto47f3ybpV3nKeTY4M52Hcwpely1bpksvvbQ3qUJP2b5m9rnIwzojD39v5cqVXc23YsWK2WcaoG7zUOp9PnZ7zDC70047rasHISxevFhHH330vLdTt+9vk/SrPOU8G5yZzkOaDQAAAKAYBK8AAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAaiUipjpN23qQCQEAABgVK1asGHYSijUxMbGm0zRqXgEAAFAMglcAAAAUg2YDqLVVq1Zp06ZN81p2bGxMxx13XI9TBADDs5AycZSMj4/vP+w0oH+oeUWtLaSQpoAH0DSUa92xTXzTYGQuAAAAikHwCgAAgGIQvAKonbGxsZ7MAwBoHjpsAagdOtoBADqh5hUAgIqZnuwDYPioeQUAdDSKTwia6ck+AIaPmlcAAAAUg5pXAACAIRuFB1D06uFB1LwCAAAMWdMDV6l3+0jwCgAAgGIQvAIAAKAYBK8AAAAoBh22ANRONx0XetXwHwBQFmpeAdRON436R6FzAwBgSwSvAAAAKAbNBgAAGBElPTFt5cqVw04CaoqaVwAAABSD4BUAAADFIHgFAABAMQheAQAAUAw6bAEAABSmDp3vhtWpjppXAAAAFIPgFQAAAMUgeAUAAEAxCF4BAABQDIJXAAAAFIPgFQAAAMVgqCwAwECsWrVKmzZt6uk6x8bGdNxxx/V0nQDqjZpXAMBA9Dpw7dc6AdQbwSsAAACKQbMBdK0ft/z6bdBP/+AWJoAmWkj5T7mIXqPmFV0rLXAdBo4RgCZaSNlGuYheo+YVAACgweZSc15CTTnBK1CYycnJnjWH6KaQqnNzkbodh/Hx8f17kiAA6KG5lOF1Le+raDYAFGZqaqpn6+qmkCqhIFuoXh0H25SpANBnFLQAAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK9AYRYt6t1pOzY21pN5Ster4xARvXuCBACgLR4PCxRmfHxcK1asGNj2evmM67k8znWQ+9iNbo7DxMTEmgEkBQB69njsElHzCgAAgGJQ84qujY2NjcRz7hdiFG6xAyjXKNfWoTkIXtG1Xt4+7tZCC9q63XoGAAALQ7MBAAAAFIOaVwBAR9xmBlA31LwCAACgGASvAAAAKAbBKwAAAIpB8AoAAIBiELwCAACgGASvAAAAKAbBKwAAwJDV5QmNdUnHTBjnFQAAYMiG8RTLUlHzCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAikHwCgBAIUroCd6qxDSj3hhtAACAQtAjHaDmFQAAAAUheAUAAEAxCF4BAABQDIJXAAAAFIPgFQAAAMVgtAEAAIA+WLly5bCT0EjUvAIAAKAYBK8A0ACjOhD8qO43MMpoNgAADcDg9QBGBTWvAAam21oyatMAAJ1Q8wpgYKgdBAAsFDWvAAAAKAbBKwAAAIpB8AoAAIBi0OYVtTY2NqZNmzbNe1kAqCsGsAfmh+AVtUYHHwAAUEWzAQAAABSD4BUAAKALNEdbmF4dP5oNAAAAdIGmbPVAzSsAAACKQc0rgIFZtWpVV6NHjI2NUcMBAGiLmlcAA9PtsGfzHR4NANB8BK8AAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYjojuZ7ZvlHRN/5KDBdgnInabbSbysNYan4fj4+P72571ojkipiYnJ9cMIk091lUeSmXn4wjoKh+33377qSVLlngQCcLcrF+/PjZu3DhrWcN5WGsdz8M5Ba8AAADAMNFsAAAAAMUgeAUAAEAxCF4BAABQDIJXAAAAFIPgFQAAAMUoJni1/UXbRy5g+bW2n9bLNAEAgMT2Mtthe+thpwXNNvDg1faBtr9te4PtW2xfbPvxsy0XEQdHxOk9SsOjbH/Z9k22txgrzPaxti+1fbft1XNc93/a/rntKdvL57LdJskXC5tsL235fE0u3Jb1absvzMd/g+3f2D7d9s5dLnui7XW2b7V9je039SONpRhWHuZtPMT252zfls+XE+ew7NNsf9/2Hbavs/23/UpnHQzxXKMcHTLbt1deU7bvrLw/fNjpQ//lc3y/ls8mbJ85rDQNwkCD1xxEfE7SKZJ2lbSHpJWS7l7geud6lXePpLMl/X2H6TdIeqek/5pHcn4o6ZWSvj+P7TbN1ZJeNP3G9qMlbd/nbV4s6U8jYrGkh0jaWikvu/EhSQ+LiJ0lPVnS4baf259kFmPgeWh7TNJXJH1d0rikPSV1VRDbfoSkj0p6s6TFkh4j6bL+pLRWhnGuUY4OWUTsOP2SdK2kQyqfnTXs9AH9Muia1z+UpIj4WETcFxF3RsQFEXG57eW5FvZ9udbsZ7b/cnpB2xfaPir/Pz3ve2zfLGnC9r62v2775nxFfpbtJe0SERE/j4gPSfpJh+nnRcT5km5unWb7Dba/Mx0w236F7Z/Y3i4v+/6I+Jqku+a63QY6Q9JLK++PlPSR6Te2n51rh27NNZ4TlWnva6lVuHd6uu3dbZ9r+0bbV9t+9fRyEbEuIm6qbPM+Sfvl5fZ1qu1/XGU9N9o+KC/784i4o7Ls1PSyI2zgeShpuaQbIuLdEXFHRNwVEZfn5WbMQ0lvkXRaRHwxIu6NiJsj4sqeH5X6Gca5RjlaU601b265nZ9/T9+Rf0dvs32Bc819Zd4jbV+bf0/fXFnXIttvtH1l/r092/aug99LjLJBB6+/kHSf063cg23v0jL9AElXSloqaYWk82Y4KQ6QdJWkB0o6XpIlrZK0u6SHS9pL0kTP90D6V6Wa4rfY/gNJJ0g6IiK2KGShSyTtbPvhtreS9EJtXoN2h9IP7hJJz5b0CtuHSVJEHFupUThQ0m8lfdrp0aKfVaqZ2UPSX0p6je1nTK/UqWnKBkm3SfobSSfldV4p6Q2SzrS9vaQPSzo9Ii6sLPtG27dLuk7SDkq1eKNsGHn4RElrndq535R/aB+d1zlbHj5Rkmz/yPavbJ85Ij+sQznXFoBydPheLOllkh4gaUzS61umHyjpoUr5/jbbD8+fv0rSYZL+XOn39reS3j+IBAPTBhq8RsStSidESPqgpBttf8b2A/Msv5F0UkTcExGfkPRzpYK2nRsi4pRcu3JnRPwyIr4SEXdHxI2S3q10cvV6H6aUfgReLekzkk6MiBKfwT4o0zVCT5f0U0nXT0+IiAsj4kcRMZVr1j6mljyzvZuk8yW9KuWEl5gAAB5nSURBVB/nx0vaLSLeHhGbIuIqpe/SCyvrvSg3G9hT6UdybWXaByX9UtJ3JD1I6fayKtPfJWknSY/Lad/Qi4NQuEHn4Z75//cq/Th+XimYGsvbnCkP95T0EqWLlj+QdD+lZkqjYODn2nxRjtbChyPiFxFxp1IzjMe2TF+Zf1t/qHQB85j8+TGS3hwR10XE3UqVRM8znbQwQAP/skXET5VuC8r2w5RqB06S9GVJ10dEtQH+NUo/Xu2sq77JAfDJkp6iFHwsUroi7LmIWGv7vyU9S1xxzuYMSd+U9GBVbmNKku0DJL1L0qOUrvy3lfTJyvRtJJ0j6aMR8fH88T6Sdre9vrKqrSR9q3XDEXG97S9J+rhSMDrtg0o/mP+QC9/W5ULSmlzDtFLSa+eyww006Dy8U9JFEfHFvI5/U2oO8HClH1Gpcx7eqfyjnJc9QdJX573nZRnauTYflKNDN1n5f6OkHbucvo+kT9meqky/T+kuKAbvPknbtHy2jVLb8MYa6lBZEfEzSauVClRJ2sO2K7PsrdTov+3iLe9PyJ89One4OUKpKUHP2X62pCdJ+ppSzR46iIhrlDqTPEvSeS2TP6oUgOyVa0pP1eZ5doqkW5UCl2nrJF0dEUsqr50i4lkdkrC1pH2n39jeUeli6UNKbaVnuqW82bKjagh5eLm2PL9/Z5Y8bF12ZHqj1+BcmxPK0b66Q5t32Bvv4brXSTq45XuxXURcP+uS6IdrJS1r+ezBSpV/jTXo0QYeZvt1tvfM7/dS6iF7SZ7lAZJebXsb289Xqmn5Qper30nS7ZI22N5D0j/NkA7njgFj+f12tretTN86T99K0lZ5+nRD96WS/p+ko5Q6RRxi+1mVZcfyspa0TV52UTfbbbC/l/QXsXlnKCnl2S0RcZftJyi1wZIk2T5a6bbm4fkW47TvSrrNqcPH/Wxv5TR0zuPzcofb3jv/v49Se+ivVZY/WdKlEXGU0u3oU/O8i2wfbXuXnE9PkPSPLcuOsoHlodLdmCc6DXm1laTXSLpJ6Va41CEPsw9LepnTUFvbS3qj0ggno2KQ5xrlaH39QNKf2d7b9mJJx/Vw3adKOj6Xr7K9m+1De7h+zM0nlNqO75l/x54m6RClOynNFREDeyk1+j9bqS3WHfnvaZJ2VmpKcLGk9ym1M/yFpL+qLHuhpKPy/8uVbitW1/1IpSFxblc6cV8n6brK9LWSnpb/X6ZUI1N9ra3MO9Fm+kSedp6kUyvzHqxUO3z/Sjpblz2om+026VU93i2fb533e5mk5yldHd6mFGC8T9KZleN4d87P6deb8rTdldrsTSo1DbmkkrfHK3W2uiP//c9K3hyav3O75vc7KrWdPFzpQu5Lkm7J2/qFpDdJ8rCP5ajlYZ7+3Jw3t+b1PHK2PKwsu1LSjfl1hqRdhn0sm5hPohyt1av1e6DUFGN9Pj9eno/T1pXje1Rl3uXKv6mV47t1Zfrv5lcqK1+r1CflNqVO1id0WpZX3/P9fvp9344NSsPL/Z9hp6vfL+edHzqngaiPiogDh50WAAAA1FMxj4cFAAAACF4BAABQjNo0GwAAAABmQ80rAAAAijGnhxTssMMOsXjx4hnnWbRokcbHezmkHLpx2WWX3RQRu80239KlS2PZsmUDSBHmqsQ8nJyc1NTU1Owzjoj169fHxo0bu6oUqFM+YnMlnovYHHlYvpnycE7B6+LFi3X00UfPOt+KFSvmslr0gO2uBiRetmyZLr300n4nB/NQYh6uXLly2EmoldNOO63rB6PUKR+xuRLPRWyOPCzfTHlIswEAAAAUg+AVAAAAxSB4BQAAQDHm1OYVAAB0tmrVKm3atGnYyRiosbExHXfccb973+0xmO9ys60HzUfNKwAAPTJqgau05T53ewzmu9xs60HzEbwCAACgGASvAAAAKAbBKwAAAIoxlA5bo9igvd/Gx8f3H3YaAHRvcnKShzwMGB17gGYYSs0rgWvv2aYWHSgIj9UdPH57gGZgqCwAAPqom0emt6uFr+Oj1rlbgDqgtg4AAADFIHgFAABAMQheAQAAUAzavAIAUIBePj61SaP+jI2NDTsJGDCCVwAACtDLx6eWFLjWseMahotmAwAAACgGNa/AiGrSbcO6iAgGbwWAPiN4BUZUnQLXptwWnJiYWNPtvIsWceMLAOaj9sFrU37U5osBoYFmGh8fp3wbkfKtdT95TC2wMLUPXgEAaJJe3/WoXgQt5IJgvk8CAwaN+1YAAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYdNgCAAC1NZ9OYuPj4/v3ISmoCWpeAQBAo9gmvmkwMhcAAADFIHgFAABAMYpv81ry89l5ygqAmTS9fBsbGyt2/wAMT/HBa8kFX8lpB9B/JZcR3aS9lxfvdXnyUzcB+djY2IBSMzdNeoxtREwNOw3on+KDVwAA6qLUYK+dulw8dfPY2lYTExNr+pAU1ARtXgEAAFAMglcAAAAUg+AVAAAAxaDNK4aiTr2oS+6UsBB16endi84rvfg+jer3APM3OTlZm45ic1Vyx7LW832Q526dfrtK1Ku8InjFUNTp5K9TWgapSYFaL/JwVL8HmL+pqXI7tJd8/reeq4M8dyknFqZXx49mAwAAACgGwSsAAACKQbMBAKipurRLnq9etQelPTKAqpEKXucz0PF8ldqIH0B91DFgG0bZVnIAD6D3Rip4BQAA3fWap8YbdUXwCgBAH3Vz12/QNdrd1GZT4426osMWAAAAikHwCgAAgGIQvAIAAKAYBK8AAMzDokWz/4TW9RGr3Sg57Wg2OmwBADAP4+PjAx2Csd+atC9oNmpeAQAAUAxqXisY9w7AoExOTtbuYSZ1Ld+4fQ2giuC1gnHvAAzK1NTUsJOwhV6Xb9yGBtAPBK8AAPRIN3fw+mlsbGyL7VNzjaYheAUAoEeGfXeujs0+gF6jwxYAAACKQc0rUJg6dvQBAGBQqHkFClPHjj4AAAwKwSsAAACKQbMBAAAK1qtmRHVtjlTXdGF4CF4BAOijbsa7JUADujdSwSuFA4C6WLSofq22Bj0e6LDHRAVQppEKXgGgLsbHx0f+CVQErgDmo36X/gAAAEAHBK8AABSgro957VW65rueuh4X9A/NBgAAKEDTH/3a9P1D71DzCgAAgGIQvAIAAKAYBK8AAAAoBm1eAQAYoG7Ht+3n2ORjY2ObtTGtw5i7vUzD+Pj4/j1ZEWqJmlcAAAZo2EFiuzTUMU0LYZv4psGKr3kdGxsb6EnHkBwABqUOtWH9NOjyG0AzFB+8MrQGgKZqemDXy/Kbx38Do4NqdQAAABSD4BUAAADFIHgFCrNoEadtP9CeHcBsKCcWplfHr/g2r8CoGR8f14oVK4adjJ7pVVvFJh0TjKZefId73fa3mzSNUnvjuvSzmc8xb1IZSRUOAAAAikHwCgAAgGIQvAIAAKAYtHkFAKBmSnpAxXwfNtHPzk8RMdW3lWPoah+8jlJDcACoKv0JVL0qv8fGxmrTUWZQSsr3fufNfDoaTUxMrOlDUlATtQ9eAWBU1TFgG0aFQkmBHID+o80rAAAAikHwCgAAgGLQbAAAgBHTbWepOvY7ae3MNoptokcdwSsAAA3SpCcptdPaBpo20aOHZgMAAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK8AANTMfB+d2s9HrgJ1MZTRBkp/5GGvUMgAo2tycrJ2wxAx5FB9kA9AZ0MJXjkpgeFrHStxPnoR7PTiYrYXF4K9OB7j4+P7dzvv1NTUgrbVD1QqACgB47wCI6oXgUov1lGXi9le7IttmmIBQJ9R0AIAAKAY1LwCAFADg2wDXUr7ZvqGoB2CVwAARky3zWSa/qhZlIlmAwAAACgGwSsAAACKQfAKAACAYtDmFQAAtFW3B2kAUp+CV77sADCzRYvqd+Or5J7d3TzsouT9A/B71LwCwBCMj4/Tk7uHShj2CUBv1O/SHwAAAOiA4BUAAADFIHgFAABAMQheAQDokdZOYe06idWh41g36SxFyWnH/NBhCwCAHumm41gdO5fVMU1AJ9S8AgAAoBgErwAAACgGwSsAAACK4Yjoeubtt99+asmSJe5jejBP69evj40bN856MWL7RknXDCBJMxofH9/fdi0uniJianJycs2w0yFpn4jYbbaZepWHvciDGh27BevF8ej2PJTqcy7O1TDO3SF8zwZ6LqIvGp2Hcz0PCy2rO+bhnIJXAAAAYJhqUfMFAAAAdIPgFQAAAMUgeAUAAEAxCF4BAABQDIJXAAAAFIPgFY1me7nti2aY/hTbPx9kmurG9jLbYXvkHhfduu+2L7R91LDT1Usl5q/tU22/ddjpaLoSvxuA1KDg1fbtldeU7Tsr7w8fdvqwuVxg7tfy2YTtM/P/B+V5PtUyz2Py5xf2YrsR8a2IeOh81lUi22tbzo3bJe3e5bJfrCx3j+1Nlfen9jidB9r+tu0Ntm+xfbHtx/dyG01UUP5W0/lb25+3vdf09Ig4JiLe0cttNpntF9r+ju07bP8m//9K23Mal72JF29opsYErxGx4/RL0rWSDql8dtb0fFxhFuVGSU+yff/KZ0dK+sWQ0tMUh7ScLzd0s1BEHFxZ5ixJJ1bWc8z0fAs9x2zvLOlzkk6RtKukPSStlHT3QtY7Qmqdv63plPQgSb9Wym/Mke3XSTpZ0r9KGpf0QEnHSPpTSWNDTBrQN40JXjvJNXjX2X6D7UlJH7a9yPYbbV9p+2bbZ9vetbLME3Otz3rbP7R90PD2YKRtknS+pBdKku2tJL1A6YdV+bMtbnt1qj2w/c387w9zjc8Lpr8ffd2Lgth+vu3LWj57re1Pz7Jc2P5H21dIuiJ/drLtdbZvtX2Z7adU5p+wfY7tT9i+zfb3bT8mT/5DSYqIj0XEfRFxZ0RcEBGX52WX55rY9+Rz9CrbT86fr8s1T0dWtvVs22tyOtbZnujJwSpQTfJ3MxFxl6RzJD2isvxq2++svP9n27+yfYPto9zmzs0osr1Y0tslvTIizomI2yJZExGHR8Td3X7/bR8v6SmS3pfLx/flz59s+3tOd0G+Z/vJlWUutP2OfD7eZvsC20sHsOsYcY0PXrNxpRqcfST9g6RXSTpM0p8r3VL7raT3S5LtPSR9XtI78zKvl3Su7VkfM4e++Iikl+b/nyHpx+qyJqlVRPxZ/vcxuTbpEz1IX9N8RtKDbT+88tlLlPJhNodJOkC/D0K+J+mxSufRRyV90vZ2lfkPlfTJyvTzbW+jVLN+n+3TbR9se5c22zpA0uWS7p+X/bikx0vaT9IRSj/AO+Z571D6Di2R9GxJr7B9WBf700R1yN/N2N5e6aL0knYbtf1MSa+V9DSl/D2oi7SOiidJ2lbSTBcfXX3/I+LNkr4l6dhcPh6bK3U+L+m9SufauyV93pvfDXuxpJdJeoBSTe/rF7xXwCxGJXidkrQiIu6OiDuVbqm8OSKui4i7JU1Iel6uvTtC0hci4gsRMRURX5F0qaRnDSvxoywivi1pV9sPVSqAu/mRxczOzzWW622fX52Qz4dPKJ0Hsv1IScuUbuPPZlVE3JLPMUXEmRFxc0TcGxH/rvQjW21ffFmuLbpH6UdxO0lPjIhbJR0oKSR9UNKNtj9j+4GVZa+OiA9HxH05vXtJens+xy9QqrXfL6fjwoj4UT6fL5f0MaUL16aqdf62plPSBklPV7rt3c7fSvpwRPwkIjYqlddIlkq6KSLunf7Av79reKftP1vg9//Zkq6IiDNyPn9M0s8kHVKZ58MR8Yv8vThb6YIG6KtRCV5vzLempu0j6VPTBbykn0q6T6mt0D6Snl8p/Ncr/ZA+aOCpbrb7JLXWwmwj6Z42854h6VhJT5X0qTbTMTeHRcSS/GpXA3m6pBfbtlKt3Nk56JnNuuob26+3/dN8u3G9pMVKP7ZbzB8RU5KuU+5cFBE/jYjlEbGnpEflz0+qLPvryv/TwVTrZzvmdBxg+79t32h7g9LFa5NvbdY+f6vpVApqj5X0Ddvjbba7e8u217WZZ1TdLGmpK82mIuLJ+bjeLGnRAr//u0u6puWza5TaoU+brPy/Ufm8A/ppVILXaHm/TtLBlQJ+SURsFxHX52lntEzbISLeNfhkN9q1SjU+VQ/WlgWllILXVyrViG9smXZH/rt95bN2P4DoUkRcolRz+RSlW4JndLvo9D+5/eM/K9Wa7ZJ/TDdIqvZ+3qsy/yJJe6pNk5CI+Jmk1UpB7Hx8VOl2+V4RsVjSqS3pGCk1zN/7IuI8pQvaA9ts91d52S3WC/2PUkfGQ2eYZy7f/9bfyhuUKnSq9pZ0/dyTCvTOqASvrU6VdLztfSTJ9m62p0/+MyUdYvsZtreyvZ1Tp549O64N8/EJSW+xvadTB7qnKd2KOqd1xoi4Wuk215vbTLtRqSA9IufX30nad4bt/lrSQ3qxAw33EUnvk3RPRHQcJ3cGO0m6V2nEiK1tv03Szi3z/LHt5+Zao9co/QhfYvthtl83fc45DaH0InVoE9llWm6JiLtsP0EpYBt1Q8vf1hU5OVTSLkp3wVqdLellth+e28cy/msWEeuVRuL4gO3n2d4pl6ePlbRDnm0u3//W8vELkv7Q9ottb237BUptnrtpZgL0zagGrycrXYleYPs2pQL1AEmKiHVKV7FvUiqY10n6J43useqXt0v6tqSLlDrMnSjp8Ij4cbuZI+KiiOjUUevlSnl0s6RH5vV2MiHp9Nwk5G/nmfZRcIZSTeeZ81z+y5K+pNT56hpJd2nL272fVuqo81ul29fPze0jb1M6H79j+w6l8/PHkl43z7S8UtLb87n+NqVgaNQNM3+nfdZpHNpbJR0v6ciI+EnrhiLii0odhv5b0i/1+wCYodMkRcSJSh3a/lkp+Py1pNMkvUGpLJzL9/9kpf4fv7X93oi4WdJzlM69m/M2nhMRN/Vrf4BuOKL1LgGAUWf7fpJ+I+lxEXFFH9Y/IWm/iDii1+vG7ErO3zxSwo8lbVvtqARgdFCbCKCdV0j6Xj8CG9RCUflr+69tb5uHTfsXSZ8lcAVGF0+bArAZ22uVOnSM6liojVZo/h6t1GnvPknfULoVDmBE0WwAAAAAxaDZAAAAAIoxp2YDO+ywQyxevLhfaVmQRYsWaXx8dIf3vOyyy26KiFkfYbt06dJYtmzZAFKEueo2D+d7Ho76OTJXk5OTmpqamtMy69evj40bN3ZVKdBNPpJnvddNvnabj5Sn9dVteYoyzSl4Xbx4sY4++uh+pWXBVqxYMewkDI3tdoP7b2HZsmW69NJL+50czEO3ebiQ83CUz5G5Wrly5ZyXOe2007p++EG3+Uie9VY3+dptPlKe1le35SnKRLMBAAAAFIPgFQAAAMVgqCwAANBTq1at0qZNm/q2/rGxMR133HF9Wz/qjeAVAGpsPm1vB6U1gOh3wIJy9Pt7wPdstI1M8Nr0QnV8fHz/YacBwGhpLVObXMYCqI+RafPa9ELV9sjkJQAAGF0jU/OKzppeK11Hw2qvVYdb0DPtO99FoBnGxsY4l9E3BK+ggBmCUT7mM+37KB8XoEkWenFehwtt1Be3mgFgCBYtovitq4iY26PVAAzUyNS8cgsDQJ2Mj49v9vSsTjVNdXnC1kJqwuqyD1L7/WhN38TExJpBpQfA3I1M8MotDAAAgPJx3woAAADFIHgFAABAMQhegcLQ0ae+6OgDAP3XqDavC2mXynOSt1SnThYl63V76daOPnPZ1iDztB/txOuS/k7poKMPMDgznaM8dbLZqMLJGIkAAIBm4KmTzUbmAgAAoBgErwAAACgGwSsAAGgUOk82W6M6bAEAgNEwUwdOOk82GzWv2djY2IKmAwAAoP96XvPa1OGVeLystGrVKkZlmAVDrgEA0F80G0DXCFxnxzECUBf9rHDgQh3DRLMBAAAaqJ8X01yoY5gIXgEAAFCMgTYbqHObydlugdQ57QAAtBobGxva7xa/meingQavdf4iz5a2OqcdAIBWC2mTutBOxvxmop9oNgAAAIBiELwCAGbVbqzr1s+6mQcAFoqhstATTR3ft5MmjNsLzEU3t6AZOgnT+t3eloui0UbwmnEiAADQG1zIoJ8aFbyOWu0fAADAqKHNKwAAAIrRqJpXAAAwGHVu+z8+Pr7/sNOA/qHmFQAANIpt4psGI3MBAABQjIE2G+BRdaOjScd7tkcHo71O5/tMI3vMVEYwIggAQBpw8Mqj6kZHk453k/ZlkOZzvnORAKAXImJq2GlA/9BhCwAAzFmdh6ecmJhYM+w0oH9o8woAAIBiELwCAACgGD1vNjDftqn97hQzzM5ig0D7HgCol353XK1zZ9JR3nf0X23avPY7sGz6l5z2PQBQL/3+Xatzhcwo7zv6rzbBK4D+6/UTcepS+9GkodkAADMjeMVA1bl3qlTvxx3WUV0CxrqkAwDQfwSvAAD0wbD7WgzzjsQg9n2myobx8fH9+7pxDFWjgtdRrjXjRAWAellok5qSH84z7H23zWhKDdao4HWUcaICzdSLi/K6tE1eCNo1A5hGwAMADdeEoK8J+wCgNwheAQBooLGxsSLX3Yv1M/Z5s9FsAACABiq5qchC087Y581G8IqhGnZv2JIL93YG3bu537Uv3RrkftdlnwFgVM0peF20qH+tDGb7QRj2kCPoj2HmaRO/T00LxrvVhP3uZxnXhIC7X8en9GMzyqPszIQReJptTsHr+Pj40AaZH/aPEwUEgH4adhlXdxwfzAUj8DQbmQsAAIBiELwCAACgGASvAACgURgqq9nm1OZ1cnKyb20/Z+v5zdNVADRZP8u4Joys0a/jU/qxGVY/lLpjqKxmm1PwOjXVvwuZ2QolAtdmGuYoEqX3Mm5n0Bd5dfnhH+R+92uf+5n+JpSf/dqHJhwbYNQwziuGaqFBQKc7AaNaGzHoH+K6/PAPMh112WcAGFUErwAANFDJTVEWmnbGeW02OmwBANBAJTdFWej6Gee12cjchqBnJYBOmtC+uwn7AKA3GtVsYFTbOUr0rASaapTLtape3aLmaYlA+RoVvAIAUBfDHuJxmKO5DHvfuRvZbASvAAD0wbBHplhIbfVCa6gHse8z3ZXgbmSzEbxioOZbIM61Zyu3BgejLu0Qh1nDBAAYLIJXFIHApDea2n6yH0P2cAEEAPVUm+C13zU4w25/02+MaQcA9dLvOwJ1ufPRzijvO/qv58FrXWt2mhy4SoxpBwB1U4dHJw/LKO87+o+ABwAAAMUgeAUAAEAxatPmFQAAlKPOnRrpB9JsAw1eh9lpiqF0BqvXx3uYjfPpGDA/nc73mYY9m6mMmOtwaQBGF/1Amm2gweswg8eF/ujV+QqzjoYdZNS14+Ao6XS+z1QOzHcaAGB0cGUCAACAYhC8AgCARomIqWGnAf1Dhy0AADBndW6eNTExsWbYaUD/NCp4XUi7VDqDAAAA1F+jgteFoDMIAAC90e/RhahwGm0Er+iJfo/GMNeCitEhAGB4+l0hRIXTaCN4RREoqIDhaleT1npR2c08ALBQjDYAAJhVuwvI1s+6mQcAFoqaVwAAGoinWqKpBhq81vnLPNsjQOucdgAAWvFUSzTVQIPXkts9cSICAAAMH21eAQAAUAyCVwAAGmi25nB1XTcwGzpsoWvDbPdbSkFZSjoBNF/JTfWAmfQ8eC21bedsYxEOs9dmXZRUENb5mdsAAGD+aDaQzRaYjnrgCgAAUAc0GwAAAMWZ6U7v+Pj4/gNMCgaMmlcAANAotolvGozMBQAAQDEIXjN6iQMAANRfo9q80sO8t+o0csRso0G0qlPae21ycnLe+1f6calL+julg3Z2QD1ExNSw04D+aVTwiuZitIffm5qiTK4r2tkBgzNThdXExMSaASYFA0ZBCwAAgGIQvAIAAKAYBK8AAAAoxsi0eeXxrgAAAOUbmeCVwBVAnXQ7akRdRlhYiLrvQ2v6GDUCqDeaDQDAEDBqRH0xagRQbyNT84rOxsbGal8z3bSHSDRtf+Zipn0v4bsIYHY01UM/EbxiToP/l4IHVrRX9+NSp+9i3W91A3VG4Ip+GplbI02v6eJpIgAGrbVcbXo5C6AeRqbmtU41Ov3A00SAZqp7bXlVCeUsNeqD0e8mQFwojbaRCV4BAMBglHAhg3KNTLMBAAAAlI/gFQAAAMVwRHQ98/bbbz+1ZMkS9zE98xYRU5OTk6Pc7nOfiNhttpls3yjpmgGkZyDGx8f3bx2TseDvQld5ON/zsODjMhTtvluzWb9+fWzcuLGrZbrJR/Ks97rJ127zsWnlacN0VZ6iTHMKXgEAAIBhotkAAAAAikHwCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAikHwCgAAgGIQvAIAAKAYBK8AAAAoBsErAAAAivH/A7bwdOuScgUpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 17 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "3nLaKviAXrq9",
        "colab": {}
      },
      "source": [
        "#@title Implement the goal-conditioned actor and critic.\n",
        "def disable_training_actor(actor):\n",
        "  for layer in list(actor._mlp_layers)[1:-1]:\n",
        "      layer.trainable = False\n",
        "\n",
        "def disable_training_critic(critic):\n",
        "  for layer in list(critic._joint_layers)[1:-1]:\n",
        "      layer.trainable = False\n",
        "\n",
        "def set_goal(traj, goal):\n",
        "  \"\"\"Sets the goal of a Trajectory or TimeStep.\"\"\"\n",
        "  for obs_field in ['observation', 'goal']:\n",
        "    assert obs_field in traj.observation.keys()\n",
        "  obs = traj.observation['observation']\n",
        "  tf.nest.assert_same_structure(obs, goal)\n",
        "  modified_traj = traj._replace(\n",
        "      observation={'observation': obs, 'goal': goal})\n",
        "  return modified_traj\n",
        "\n",
        "def merge_obs_goal(observations):\n",
        "  \"\"\"Merge the observation and goal fields into a single tensor.\n",
        "\n",
        "  If both are 1D, we concatenate the observation and goal together. If both are\n",
        "  3D, we stack along the third axis, so the resulting tensor has\n",
        "  shape (H x W x 2 * D).\n",
        "\n",
        "  Args:\n",
        "    observations: Dictionary-type observations.\n",
        "  Returns:\n",
        "    a merged observation\n",
        "  \"\"\"\n",
        "  obs = observations['observation']\n",
        "  goal = observations['goal']\n",
        "\n",
        "  assert obs.shape == goal.shape\n",
        "  # For 1D observations, simply concatenate them together.\n",
        "  assert len(obs.shape) == 2\n",
        "  modified_observations = tf.concat([obs, goal], axis=-1)\n",
        "  assert obs.shape[0] == modified_observations.shape[0]\n",
        "  assert modified_observations.shape[1] == obs.shape[1] + goal.shape[1]\n",
        "  return modified_observations\n",
        "\n",
        "\n",
        "class GoalConditionedActorNetwork(actor_network.ActorNetwork):\n",
        "  \"\"\"Actor network that takes observations and goals as inputs.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_tensor_spec,\n",
        "               output_tensor_spec,\n",
        "               **kwargs):\n",
        "    modified_tensor_spec = None\n",
        "    super(GoalConditionedActorNetwork, self).__init__(\n",
        "        modified_tensor_spec, output_tensor_spec,\n",
        "        fc_layer_params=(256, 256),\n",
        "        **kwargs)\n",
        "    self._input_tensor_spec = input_tensor_spec\n",
        "    self.create_variables()\n",
        "\n",
        "  def call(self, observations, step_type=(), network_state=()):\n",
        "    modified_observations = merge_obs_goal(observations)\n",
        "    return_vals = super(GoalConditionedActorNetwork, self).call(\n",
        "        modified_observations, step_type=step_type, network_state=network_state)\n",
        "    return return_vals\n",
        "\n",
        "\n",
        "class GoalConditionedCriticNetwork(critic_network.CriticNetwork):\n",
        "  \"\"\"Actor network that takes observations and goals as inputs.\n",
        "\n",
        "  Further modified so it can make multiple predictions.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_tensor_spec,\n",
        "               observation_conv_layer_params=None,\n",
        "               observation_fc_layer_params=(256,),\n",
        "               action_fc_layer_params=None,\n",
        "               joint_fc_layer_params=(256,),\n",
        "               activation_fn=tf.nn.relu,\n",
        "               name='CriticNetwork',\n",
        "               output_dim=None):\n",
        "    \"\"\"Creates an instance of `CriticNetwork`.\n",
        "\n",
        "    Args:\n",
        "      input_tensor_spec: A tuple of (observation, action) each a nest of\n",
        "        `tensor_spec.TensorSpec` representing the inputs.\n",
        "      observation_conv_layer_params: Optional list of convolution layer\n",
        "        parameters for observations, where each item is a length-three tuple\n",
        "        indicating (num_units, kernel_size, stride).\n",
        "      observation_fc_layer_params: Optional list of fully connected parameters\n",
        "        for observations, where each item is the number of units in the layer.\n",
        "      action_fc_layer_params: Optional list of fully connected parameters for\n",
        "        actions, where each item is the number of units in the layer.\n",
        "      joint_fc_layer_params: Optional list of fully connected parameters after\n",
        "        merging observations and actions, where each item is the number of units\n",
        "        in the layer.\n",
        "      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n",
        "      name: A string representing name of the network.\n",
        "      output_dim: An integer specifying the number of outputs. If None, output\n",
        "        will be flattened.\n",
        "\n",
        "    \"\"\"\n",
        "    self._output_dim = output_dim\n",
        "    (_, action_spec) = input_tensor_spec\n",
        "    modified_obs_spec = None\n",
        "    modified_tensor_spec = (modified_obs_spec, action_spec)\n",
        "\n",
        "    super(critic_network.CriticNetwork, self).__init__(\n",
        "        input_tensor_spec=modified_tensor_spec,\n",
        "        state_spec=(),\n",
        "        name=name)\n",
        "    self._input_tensor_spec = input_tensor_spec\n",
        "\n",
        "    flat_action_spec = tf.nest.flatten(action_spec)\n",
        "    if len(flat_action_spec) > 1:\n",
        "      raise ValueError('Only a single action is supported by this network')\n",
        "    self._single_action_spec = flat_action_spec[0]\n",
        "\n",
        "    self._observation_layers = utils.mlp_layers(\n",
        "        observation_conv_layer_params,\n",
        "        observation_fc_layer_params,\n",
        "        activation_fn=activation_fn,\n",
        "        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "            scale=1. / 3., mode='fan_in', distribution='uniform'),\n",
        "        name='observation_encoding')\n",
        "\n",
        "    self._action_layers = utils.mlp_layers(\n",
        "        None,\n",
        "        action_fc_layer_params,\n",
        "        activation_fn=activation_fn,\n",
        "        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "            scale=1. / 3., mode='fan_in', distribution='uniform'),\n",
        "        name='action_encoding')\n",
        "\n",
        "    self._joint_layers = utils.mlp_layers(\n",
        "        None,\n",
        "        joint_fc_layer_params,\n",
        "        activation_fn=activation_fn,\n",
        "        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "            scale=1. / 3., mode='fan_in', distribution='uniform'),\n",
        "        name='joint_mlp')\n",
        "\n",
        "    self._joint_layers.append(\n",
        "        tf.keras.layers.Dense(\n",
        "            self._output_dim if self._output_dim is not None else 1,\n",
        "            activation=None,\n",
        "            kernel_initializer=tf.compat.v1.keras.initializers.RandomUniform(\n",
        "                minval=-0.003, maxval=0.003),\n",
        "            name='value'))\n",
        "    self.create_variables()\n",
        "\n",
        "\n",
        "  def call(self, inputs, step_type=(), network_state=()):\n",
        "    observations, actions = inputs\n",
        "    modified_observations = merge_obs_goal(observations)\n",
        "    modified_inputs = (modified_observations, actions)\n",
        "    output = super(GoalConditionedCriticNetwork, self).call(\n",
        "        modified_inputs, step_type=step_type, network_state=network_state)\n",
        "    (predictions, network_state) = output\n",
        "\n",
        "    # We have to reshape the output, which is flattened by default\n",
        "    if self._output_dim is not None:\n",
        "      predictions = tf.reshape(predictions, [-1, self._output_dim])\n",
        "\n",
        "    return predictions, network_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "66Rh9rum8qNh",
        "colab": {}
      },
      "source": [
        "#@title Implement the goal-conditioned agent.\n",
        "\n",
        "class UvfAgent(tf_agent.TFAgent):\n",
        "  \"\"\"A UVF Agent.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      time_step_spec,\n",
        "      action_spec,\n",
        "      ou_stddev=1.0,\n",
        "      ou_damping=1.0,\n",
        "      target_update_tau=0.05,\n",
        "      target_update_period=5,\n",
        "      max_episode_steps=None,\n",
        "      ensemble_size=3,\n",
        "      combine_ensemble_method='min',\n",
        "      use_distributional_rl=True):\n",
        "    \"\"\"Creates a Uvf Agent.\n",
        "\n",
        "    Args:\n",
        "      time_step_spec: A `TimeStep` spec of the expected time_steps.\n",
        "      action_spec: A nest of BoundedTensorSpec representing the actions.\n",
        "      ou_stddev: Standard deviation for the Ornstein-Uhlenbeck (OU) noise added\n",
        "        in the default collect policy.\n",
        "      ou_damping: Damping factor for the OU noise added in the default collect\n",
        "        policy.\n",
        "      target_update_tau: Factor for soft update of the target networks.\n",
        "      target_update_period: Period for soft update of the target networks.\n",
        "      max_episode_steps: Int indicating number of steps in an episode. Used for\n",
        "        determining the number of bins for distributional RL.\n",
        "      ensemble_size: (int) Number of models in ensemble of critics.\n",
        "      combine_ensemble_method: (str) At test time, how to combine the distances\n",
        "        predicted by each member of the ensemble. Options are 'mean', 'min',\n",
        "        and 'td3'. The 'td3' option is pessimistic w.r.t. the pdf, and then\n",
        "        takes computes the corresponding distance. The 'min' option takes the\n",
        "        minimum q values, corresponding to taking the maximum predicted\n",
        "        distance. Note that we never aggregate predictions during training.\n",
        "      use_distributional_rl: (bool) Whether to use distributional RL.\n",
        "    \"\"\"\n",
        "    tf.Module.__init__(self, name='UvfAgent')\n",
        "\n",
        "    assert max_episode_steps is not None\n",
        "    self._max_episode_steps = max_episode_steps\n",
        "    self._ensemble_size = ensemble_size\n",
        "    self._use_distributional_rl = use_distributional_rl\n",
        "\n",
        "    # Create the actor\n",
        "    self._actor_network = GoalConditionedActorNetwork(\n",
        "        time_step_spec.observation, action_spec)\n",
        "    self._target_actor_network = self._actor_network.copy(\n",
        "        name='TargetActorNetwork')\n",
        "    \n",
        "\n",
        "    # Create a prototypical critic, which we will copy to create the ensemble.\n",
        "    critic_net_input_specs = (time_step_spec.observation, action_spec)\n",
        "    critic_network = GoalConditionedCriticNetwork(\n",
        "        critic_net_input_specs,\n",
        "        output_dim=max_episode_steps if use_distributional_rl else None,\n",
        "    )\n",
        "    self._critic_network_list = []\n",
        "    self._target_critic_network_list = []\n",
        "    for ensemble_index in range(self._ensemble_size):\n",
        "      self._critic_network_list.append(\n",
        "          critic_network.copy(name='CriticNetwork%d' % ensemble_index))\n",
        "      self._target_critic_network_list.append(\n",
        "          critic_network.copy(name='TargetCriticNetwork%d' % ensemble_index))\n",
        "\n",
        "    self._actor_optimizer = tf.optimizers.Adam(learning_rate=3e-4)\n",
        "    self._critic_optimizer = tf.optimizers.Adam(learning_rate=3e-4)\n",
        "\n",
        "    self._ou_stddev = ou_stddev\n",
        "    self._ou_damping = ou_damping\n",
        "    self._target_update_tau = target_update_tau\n",
        "    self._target_update_period = target_update_period\n",
        "\n",
        "    self._update_target = self._get_target_updater(\n",
        "        target_update_tau, target_update_period)\n",
        "\n",
        "    policy = actor_policy.ActorPolicy(\n",
        "        time_step_spec=time_step_spec, action_spec=action_spec,\n",
        "        actor_network=self._actor_network, clip=True)\n",
        "    collect_policy = actor_policy.ActorPolicy(\n",
        "        time_step_spec=time_step_spec, action_spec=action_spec,\n",
        "        actor_network=self._actor_network, clip=False)\n",
        "    collect_policy = ou_noise_policy.OUNoisePolicy(\n",
        "        collect_policy,\n",
        "        ou_stddev=self._ou_stddev,\n",
        "        ou_damping=self._ou_damping,\n",
        "        clip=True)\n",
        "\n",
        "    super(UvfAgent, self).__init__(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        policy,\n",
        "        collect_policy,\n",
        "        train_sequence_length=2)\n",
        "\n",
        "  def initialize_search(self, active_set, max_search_steps=3,\n",
        "                        combine_ensemble_method='min'):\n",
        "    self._combine_ensemble_method = combine_ensemble_method\n",
        "    self._max_search_steps = max_search_steps\n",
        "    self._active_set_tensor = tf.convert_to_tensor(value=active_set)\n",
        "    pdist = self._get_pairwise_dist(self._active_set_tensor, masked=True,\n",
        "                                    aggregate=combine_ensemble_method)\n",
        "    distances = scipy.sparse.csgraph.floyd_warshall(pdist, directed=True)\n",
        "    self._distances_tensor = tf.convert_to_tensor(value=distances, dtype=tf.float32)\n",
        "\n",
        "  def _get_pairwise_dist(self, obs_tensor, goal_tensor=None, masked=False,\n",
        "                         aggregate='mean'):\n",
        "    \"\"\"Estimates the pairwise distances.\n",
        "\n",
        "    Args:\n",
        "      obs_tensor: Tensor containing observations\n",
        "      goal_tensor: (optional) Tensor containing a second set of observations. If\n",
        "        not specified, computes the pairwise distances between obs_tensor and\n",
        "        itself.\n",
        "      masked: (bool) Whether to ignore edges that are too long, as defined by\n",
        "        max_search_steps.\n",
        "      aggregate: (str) How to combine the predictions from the ensemble. Options\n",
        "        are to take the minimum predicted q value (i.e., the maximum distance),\n",
        "        the mean, or to simply return all the predictions.\n",
        "    \"\"\"\n",
        "    if goal_tensor is None:\n",
        "      goal_tensor = obs_tensor\n",
        "    dist_matrix = []\n",
        "    for obs_index in range(obs_tensor.shape[0]):\n",
        "      obs = obs_tensor[obs_index]\n",
        "      obs_repeat_tensor = tf.ones_like(goal_tensor) * tf.expand_dims(obs, 0)\n",
        "      obs_goal_tensor = {'observation': obs_repeat_tensor,\n",
        "                         'goal': goal_tensor}\n",
        "      pseudo_next_time_steps = time_step.transition(obs_goal_tensor,\n",
        "                                                    reward=0.0,  # Ignored\n",
        "                                                    discount=1.0)\n",
        "      dist = self._get_dist_to_goal(pseudo_next_time_steps, aggregate=aggregate)\n",
        "      dist_matrix.append(dist)\n",
        "\n",
        "    pairwise_dist = tf.stack(dist_matrix)\n",
        "    if aggregate is None:\n",
        "      pairwise_dist = tf.transpose(a=pairwise_dist, perm=[1, 0, 2])\n",
        "\n",
        "    if masked:\n",
        "      mask = (pairwise_dist > self._max_search_steps)\n",
        "      return tf.compat.v1.where(mask, tf.fill(pairwise_dist.shape, np.inf),\n",
        "                        pairwise_dist)\n",
        "    else:\n",
        "      return pairwise_dist\n",
        "\n",
        "  def _get_critic_output(self, critic_net_list, next_time_steps,\n",
        "                         actions=None):\n",
        "    \"\"\"Calls the critic net.\n",
        "\n",
        "    Args:\n",
        "      critic_net_list: (list) List of critic networks.\n",
        "      next_time_steps: time_steps holding the observations and step types\n",
        "      actions: (optional) actions to compute the Q values for. If None, returns\n",
        "      the Q values for the best action.\n",
        "    Returns:\n",
        "      q_values_list: (list) List containing a tensor of q values for each member\n",
        "      of the ensemble. For distributional RL, computes the expectation over the\n",
        "      distribution.\n",
        "    \"\"\"\n",
        "    q_values_list = []\n",
        "    critic_net_input = (next_time_steps.observation, actions)\n",
        "    for critic_index in range(self._ensemble_size):\n",
        "      critic_net = critic_net_list[critic_index]\n",
        "      q_values, _ = critic_net(critic_net_input, next_time_steps.step_type)\n",
        "      q_values_list.append(q_values)\n",
        "    return q_values_list\n",
        "\n",
        "  def _get_expected_q_values(self, next_time_steps, actions=None):\n",
        "    if actions is None:\n",
        "      actions, _ = self._actor_network(next_time_steps.observation,\n",
        "                                        next_time_steps.step_type)\n",
        "\n",
        "    q_values_list = self._get_critic_output(self._critic_network_list,\n",
        "                                            next_time_steps, actions)\n",
        "\n",
        "    expected_q_values_list = []\n",
        "    for q_values in q_values_list:\n",
        "      if self._use_distributional_rl:\n",
        "        q_probs = tf.nn.softmax(q_values, axis=1)\n",
        "        batch_size = q_probs.shape[0]\n",
        "        bin_range = tf.range(1, self._max_episode_steps + 1, dtype=tf.float32)\n",
        "        ### NOTE: We want to compute the value of each bin, which is the\n",
        "        # negative distance. Without properly negating this, the actor is\n",
        "        # optimized to take the *worst* actions.\n",
        "        neg_bin_range = -1.0 * bin_range\n",
        "        tiled_bin_range = tf.tile(tf.expand_dims(neg_bin_range, 0),\n",
        "                                  [batch_size, 1])\n",
        "        assert q_probs.shape == tiled_bin_range.shape\n",
        "\n",
        "        ### Take the inner produce between these two tensors\n",
        "        expected_q_values = tf.reduce_sum(input_tensor=q_probs * tiled_bin_range, axis=1)\n",
        "        expected_q_values_list.append(expected_q_values)\n",
        "      else:\n",
        "        expected_q_values_list.append(q_values)\n",
        "    return tf.stack(expected_q_values_list)\n",
        "\n",
        "  def _get_state_values(self, next_time_steps, actions=None, aggregate='mean'):\n",
        "    \"\"\"Computes the value function, averaging across bins (for distributional RL)\n",
        "    and the ensemble (for bootstrap RL).\n",
        "\n",
        "    Args:\n",
        "      next_time_steps: time_steps holding the observations and step types\n",
        "      actions: actions for which to compute the Q values. If None, uses the\n",
        "      best actions (i.e., returns the value function).\n",
        "    Returns:\n",
        "      state_values: Tensor storing the state values for each sample in the\n",
        "      batch. These values should all be negative.\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.name_scope('state_values'):\n",
        "      expected_q_values = self._get_expected_q_values(next_time_steps, actions)\n",
        "      if aggregate is not None:\n",
        "        if aggregate == 'mean':\n",
        "          expected_q_values = tf.reduce_mean(input_tensor=expected_q_values, axis=0)\n",
        "        elif aggregate == 'min':\n",
        "          expected_q_values = tf.reduce_min(input_tensor=expected_q_values, axis=0)\n",
        "        else:\n",
        "          raise ValueError('Unknown method for combining ensemble: %s' %\n",
        "                           aggregate)\n",
        "\n",
        "      # Clip the q values if not using distributional RL. If using\n",
        "      # distributional RL, the q values are implicitly clipped.\n",
        "      if not self._use_distributional_rl:\n",
        "        min_q_val = -1.0 * self._max_episode_steps\n",
        "        max_q_val = 0.0\n",
        "        expected_q_values = tf.maximum(expected_q_values, min_q_val)\n",
        "        expected_q_values = tf.minimum(expected_q_values, max_q_val)\n",
        "      return expected_q_values\n",
        "\n",
        "  def _get_dist_to_goal(self, next_time_step, aggregate='mean'):\n",
        "    q_values = self._get_state_values(next_time_step, aggregate=aggregate)\n",
        "    return -1.0 * q_values\n",
        "\n",
        "  def _get_waypoint(self, next_time_steps):\n",
        "    obs_tensor = next_time_steps.observation['observation']\n",
        "    goal_tensor = next_time_steps.observation['goal']\n",
        "    obs_to_active_set_dist = self._get_pairwise_dist(\n",
        "        obs_tensor, self._active_set_tensor, masked=True,\n",
        "        aggregate=self._combine_ensemble_method)  # B x A\n",
        "    active_set_to_goal_dist = self._get_pairwise_dist(\n",
        "        self._active_set_tensor, goal_tensor, masked=True,\n",
        "        aggregate=self._combine_ensemble_method)  # A x B\n",
        "\n",
        "    # The search_dist tensor should be (B x A x A)\n",
        "    search_dist = sum([\n",
        "        tf.expand_dims(obs_to_active_set_dist, 2),\n",
        "        tf.expand_dims(self._distances_tensor, 0),\n",
        "        tf.expand_dims(tf.transpose(a=active_set_to_goal_dist), axis=1)\n",
        "    ])\n",
        "\n",
        "    # We assume a batch size of 1.\n",
        "    assert obs_tensor.shape[0] == 1\n",
        "    min_search_dist = tf.reduce_min(input_tensor=search_dist, axis=[1, 2])[0]\n",
        "    waypoint_index = tf.argmin(input=tf.reduce_min(input_tensor=search_dist, axis=[2]), axis=1)[0]\n",
        "    waypoint = self._active_set_tensor[waypoint_index]\n",
        "\n",
        "    return waypoint, min_search_dist\n",
        "\n",
        "  def _initialize(self):\n",
        "    for ensemble_index in range(self._ensemble_size):\n",
        "      common.soft_variables_update(\n",
        "          self._critic_network_list[ensemble_index].variables,\n",
        "          self._target_critic_network_list[ensemble_index].variables,\n",
        "          tau=1.0)\n",
        "    # Caution: actor should only be updated once.\n",
        "    common.soft_variables_update(\n",
        "        self._actor_network.variables,\n",
        "        self._target_actor_network.variables,\n",
        "        tau=1.0)\n",
        "\n",
        "  def _get_target_updater(self, tau=1.0, period=1):\n",
        "    \"\"\"Performs a soft update of the target network parameters.\n",
        "\n",
        "    For each weight w_s in the original network, and its corresponding\n",
        "    weight w_t in the target network, a soft update is:\n",
        "    w_t = (1- tau) x w_t + tau x ws\n",
        "\n",
        "    Args:\n",
        "      tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update.\n",
        "      period: Step interval at which the target networks are updated.\n",
        "    Returns:\n",
        "      An operation that performs a soft update of the target network parameters.\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.name_scope('get_target_updater'):\n",
        "      def update():  # pylint: disable=missing-docstring\n",
        "        critic_update_list = []\n",
        "        for ensemble_index in range(self._ensemble_size):\n",
        "          critic_update = common.soft_variables_update(\n",
        "              self._critic_network_list[ensemble_index].variables,\n",
        "              self._target_critic_network_list[ensemble_index].variables, tau)\n",
        "          critic_update_list.append(critic_update)\n",
        "        actor_update = common.soft_variables_update(\n",
        "            self._actor_network.variables,\n",
        "            self._target_actor_network.variables, tau)\n",
        "        return tf.group(critic_update_list + [actor_update])\n",
        "\n",
        "      return common.Periodically(update, period, 'periodic_update_targets')\n",
        "\n",
        "  def _experience_to_transitions(self, experience):\n",
        "    transitions = trajectory.to_transition(experience)\n",
        "    transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]),\n",
        "                                        transitions)\n",
        "\n",
        "    time_steps, policy_steps, next_time_steps = transitions\n",
        "    actions = policy_steps.action\n",
        "    return time_steps, actions, next_time_steps\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    del weights\n",
        "    time_steps, actions, next_time_steps = self._experience_to_transitions(\n",
        "        experience)\n",
        "\n",
        "    # Update the critic\n",
        "    critic_vars = []\n",
        "    for ensemble_index in range(self._ensemble_size):\n",
        "      critic_net = self._critic_network_list[ensemble_index]\n",
        "      critic_vars.extend(critic_net.variables)\n",
        "\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "      assert critic_vars\n",
        "      tape.watch(critic_vars)\n",
        "      critic_loss = self.critic_loss(time_steps, actions, next_time_steps)\n",
        "    tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\n",
        "    critic_grads = tape.gradient(critic_loss, critic_vars)\n",
        "    self._apply_gradients(critic_grads, critic_vars,\n",
        "                          self._critic_optimizer)\n",
        "\n",
        "    # Update the actor\n",
        "    actor_vars = self._actor_network.variables\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "      assert actor_vars, 'No actor variables to optimize.'\n",
        "      tape.watch(actor_vars)\n",
        "      actor_loss = self.actor_loss(time_steps)\n",
        "    tf.debugging.check_numerics(actor_loss, 'Actor loss is inf or nan.')\n",
        "    actor_grads = tape.gradient(actor_loss, actor_vars)\n",
        "    self._apply_gradients(actor_grads, actor_vars, self._actor_optimizer)\n",
        "\n",
        "    self.train_step_counter.assign_add(1)\n",
        "    self._update_target()\n",
        "    total_loss = actor_loss + critic_loss\n",
        "    return tf_agent.LossInfo(total_loss, (actor_loss, critic_loss))\n",
        "\n",
        "  def _apply_gradients(self, gradients, variables, optimizer):\n",
        "    # Tuple is used for py3, where zip is a generator producing values once.\n",
        "    grads_and_vars = tuple(zip(gradients, variables))\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "\n",
        "  def critic_loss(self,\n",
        "                  time_steps,\n",
        "                  actions,\n",
        "                  next_time_steps):\n",
        "    \"\"\"Computes the critic loss for UvfAgent training.\n",
        "\n",
        "    Args:\n",
        "      time_steps: A batch of timesteps.\n",
        "      actions: A batch of actions.\n",
        "      next_time_steps: A batch of next timesteps.\n",
        "    Returns:\n",
        "      critic_loss: A scalar critic loss.\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.name_scope('critic_loss'):\n",
        "      # We compute the target actions once for all critics.\n",
        "      target_actions, _ = self._target_actor_network(\n",
        "          next_time_steps.observation, next_time_steps.step_type)\n",
        "\n",
        "      critic_loss_list = []\n",
        "      q_values_list = self._get_critic_output(self._critic_network_list,\n",
        "                                              time_steps, actions)\n",
        "      target_q_values_list = self._get_critic_output(\n",
        "          self._target_critic_network_list, next_time_steps, target_actions)\n",
        "      assert len(target_q_values_list) == self._ensemble_size\n",
        "      for ensemble_index in range(self._ensemble_size):\n",
        "        # The target_q_values should be a Batch x ensemble_size tensor.\n",
        "        target_q_values = target_q_values_list[ensemble_index]\n",
        "\n",
        "        if self._use_distributional_rl:\n",
        "          target_q_probs = tf.nn.softmax(target_q_values, axis=1)\n",
        "          batch_size = target_q_probs.shape[0]\n",
        "          one_hot = tf.one_hot(tf.zeros(batch_size, dtype=tf.int32),\n",
        "                               self._max_episode_steps)\n",
        "          ### Calculate the shifted probabilities\n",
        "          # Fist column: Since episode didn't terminate, probability that the\n",
        "          # distance is 1 equals 0.\n",
        "          col_1 = tf.zeros((batch_size, 1))\n",
        "          # Middle columns: Simply the shifted probabilities.\n",
        "          col_middle = target_q_probs[:, :-2]\n",
        "          # Last column: Probability of taking at least n steps is sum of\n",
        "          # last two columns in unshifted predictions:\n",
        "          col_last = tf.reduce_sum(input_tensor=target_q_probs[:, -2:], axis=1,\n",
        "                                   keepdims=True)\n",
        "\n",
        "          shifted_target_q_probs = tf.concat([col_1, col_middle, col_last],\n",
        "                                             axis=1)\n",
        "          assert one_hot.shape == shifted_target_q_probs.shape\n",
        "          td_targets = tf.compat.v1.where(next_time_steps.is_last(),\n",
        "                                one_hot,\n",
        "                                shifted_target_q_probs)\n",
        "          td_targets = tf.stop_gradient(td_targets)\n",
        "        else:\n",
        "          td_targets = tf.stop_gradient(\n",
        "              next_time_steps.reward +\n",
        "              next_time_steps.discount * target_q_values)\n",
        "\n",
        "        q_values = q_values_list[ensemble_index]\n",
        "        if self._use_distributional_rl:\n",
        "          critic_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "              labels=td_targets,\n",
        "              logits=q_values\n",
        "              )\n",
        "        else:\n",
        "          critic_loss = common.element_wise_huber_loss(td_targets, q_values)\n",
        "        critic_loss = tf.reduce_mean(input_tensor=critic_loss)\n",
        "        critic_loss_list.append(critic_loss)\n",
        "\n",
        "      critic_loss = tf.reduce_mean(input_tensor=critic_loss_list)\n",
        "\n",
        "      return critic_loss\n",
        "\n",
        "  def actor_loss(self, time_steps):\n",
        "    \"\"\"Computes the actor_loss for UvfAgent training.\n",
        "\n",
        "    Args:\n",
        "      time_steps: A batch of timesteps.\n",
        "    Returns:\n",
        "      actor_loss: A scalar actor loss.\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.name_scope('actor_loss'):\n",
        "      actions, _ = self._actor_network(time_steps.observation,\n",
        "                                       time_steps.step_type)\n",
        "      with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "        tape.watch(actions)\n",
        "        avg_expected_q_values = self._get_state_values(time_steps, actions,\n",
        "                                                       aggregate='mean')\n",
        "        actions = tf.nest.flatten(actions)\n",
        "      dqdas = tape.gradient([avg_expected_q_values], actions)\n",
        "\n",
        "      actor_losses = []\n",
        "      for dqda, action in zip(dqdas, actions):\n",
        "        loss = common.element_wise_squared_loss(\n",
        "            tf.stop_gradient(dqda + action), action)\n",
        "        loss = tf.reduce_sum(input_tensor=loss, axis=1)\n",
        "        loss = tf.reduce_mean(input_tensor=loss)\n",
        "        actor_losses.append(loss)\n",
        "\n",
        "      actor_loss = tf.add_n(actor_losses)\n",
        "\n",
        "      with tf.compat.v1.name_scope('Losses/'):\n",
        "        tf.compat.v2.summary.scalar(\n",
        "            name='actor_loss', data=actor_loss, step=self.train_step_counter)\n",
        "\n",
        "    return actor_loss\n",
        "\n",
        "  # Sets the number of trainable layers in this UvfAgent\n",
        "  def set_trainable_layers(self, n_layers, all_trainable=False):\n",
        "    '''\n",
        "    Args:\n",
        "      n_layers = number of layers from top of each network that are trainable\n",
        "      all_trainable = if true, allows full model to train \n",
        "    Ret:\n",
        "      None\n",
        "    Todo:\n",
        "      Add an assert so n_layers not too large\n",
        "      Diff number of trainable layers for each of actor and critic\n",
        "    '''\n",
        "    actor = self._actor_network\n",
        "    critics = self._critic_network_list\n",
        "    if(all_trainable):\n",
        "      # set all layers to trainable.\n",
        "      for layer in actor._mlp_layers:\n",
        "          layer.trainable = True\n",
        "      for critic in critics:\n",
        "        for layer in critic._joint_layers:\n",
        "          layer.trainable = True\n",
        "    else:\n",
        "      # set layers for actor\n",
        "      for layer in actor._mlp_layers[:-n_layers]:\n",
        "          layer.trainable = False\n",
        "      for layer in actor._mlp_layers[-n_layers:] + [actor._mlp_layers[0]]:\n",
        "          layer.trainable = True\n",
        "\n",
        "      # set layers for critics\n",
        "      for critic in critics:\n",
        "        for layer in critic._joint_layers[:-n_layers]:\n",
        "          layer.trainable = False\n",
        "        for layer in critic._joint_layers[-n_layers:] + [critic._joint_layers[0]]:\n",
        "          layer.trainable = True\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "MbGXO1TURVf-",
        "colab": {}
      },
      "source": [
        "#@title Training script.\n",
        "def train_eval(\n",
        "    tf_agent,\n",
        "    tf_env,\n",
        "    eval_tf_env,\n",
        "    num_iterations=2000000,\n",
        "    # Params for collect\n",
        "    initial_collect_steps=1000,\n",
        "    batch_size=64,\n",
        "    # Params for eval\n",
        "    num_eval_episodes=100,\n",
        "    eval_interval=10000,\n",
        "    # Params for checkpoints, summaries, and logging\n",
        "    log_interval=1000,\n",
        "    random_seed=0):\n",
        "  \"\"\"A simple train and eval for UVF.  \"\"\"\n",
        "  print('random_seed = %d' % random_seed)\n",
        "  np.random.seed(random_seed)\n",
        "  random.seed(random_seed)\n",
        "  tf.random.set_seed(random_seed)\n",
        "\n",
        "  max_episode_steps = tf_env.pyenv.envs[0]._duration\n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "      tf_agent.collect_data_spec,\n",
        "      batch_size=tf_env.batch_size)\n",
        "\n",
        "  eval_metrics = [\n",
        "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
        "  ]\n",
        "\n",
        "  eval_policy = tf_agent.policy\n",
        "  collect_policy = tf_agent.collect_policy\n",
        "  initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "      tf_env,\n",
        "      collect_policy,\n",
        "      observers=[replay_buffer.add_batch],\n",
        "      num_steps=initial_collect_steps)\n",
        "\n",
        "  collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "      tf_env,\n",
        "      collect_policy,\n",
        "      observers=[replay_buffer.add_batch],\n",
        "      num_steps=1)\n",
        "\n",
        "  initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
        "  collect_driver.run = common.function(collect_driver.run)\n",
        "  tf_agent.train = common.function(tf_agent.train)\n",
        "\n",
        "  initial_collect_driver.run()\n",
        "\n",
        "  time_step = None\n",
        "  policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
        "\n",
        "  timed_at_step = global_step.numpy()\n",
        "  time_acc = 0\n",
        "\n",
        "  # Dataset generates trajectories with shape [Bx2x...]\n",
        "  dataset = replay_buffer.as_dataset(\n",
        "      num_parallel_calls=3,\n",
        "      sample_batch_size=batch_size,\n",
        "      num_steps=2).prefetch(3)\n",
        "  iterator = iter(dataset)\n",
        "\n",
        "  for _ in tqdm.tnrange(num_iterations):\n",
        "    start_time = time.time()\n",
        "    time_step, policy_state = collect_driver.run(\n",
        "        time_step=time_step,\n",
        "        policy_state=policy_state,\n",
        "    )\n",
        "\n",
        "    experience, _ = next(iterator)\n",
        "    train_loss = tf_agent.train(experience)\n",
        "    time_acc += time.time() - start_time\n",
        "\n",
        "    if global_step.numpy() % log_interval == 0:\n",
        "      tf.compat.v1.logging.info('step = %d, loss = %f', global_step.numpy(),\n",
        "                    train_loss.loss)\n",
        "      steps_per_sec = log_interval / time_acc\n",
        "      tf.compat.v1.logging.info('%.3f steps/sec', steps_per_sec)\n",
        "      time_acc = 0\n",
        "\n",
        "    if global_step.numpy() % eval_interval == 0:\n",
        "      start = time.time()\n",
        "      tf.compat.v1.logging.info('step = %d' % global_step.numpy())\n",
        "      for dist in [2, 5, 10]:\n",
        "        tf.compat.v1.logging.info('\\t dist = %d' % dist)\n",
        "        eval_tf_env.pyenv.envs[0].gym.set_sample_goal_args(\n",
        "          prob_constraint=1.0, min_dist=dist-1, max_dist=dist+1)\n",
        "\n",
        "        results = metric_utils.eager_compute(\n",
        "            eval_metrics,\n",
        "            eval_tf_env,\n",
        "            eval_policy,\n",
        "            num_episodes=num_eval_episodes,\n",
        "            train_step=global_step,\n",
        "            summary_prefix='Metrics',\n",
        "        )\n",
        "        for (key, value) in results.items():\n",
        "          tf.compat.v1.logging.info('\\t\\t %s = %.2f', key, value.numpy())\n",
        "        # For debugging, it's helpful to check the predicted distances for\n",
        "        # goals of known distance.\n",
        "        pred_dist = []\n",
        "        for _ in range(num_eval_episodes):\n",
        "          ts = eval_tf_env.reset()\n",
        "          dist_to_goal = agent._get_dist_to_goal(ts)[0]\n",
        "          pred_dist.append(dist_to_goal.numpy())\n",
        "        tf.compat.v1.logging.info('\\t\\t predicted_dist = %.1f (%.1f)' %\n",
        "                        (np.mean(pred_dist), np.std(pred_dist)))\n",
        "      tf.compat.v1.logging.info('\\t eval_time = %.2f' % (time.time() - start))\n",
        "\n",
        "  return train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zxW1bAAPeOH6"
      },
      "source": [
        "----------------\n",
        "## Train the Agent!\n",
        "Now we're going to train the goal-conditioned RL agent. The first cell resets the weights, and the second cell does the actual training. If you want to continue training for longer, simply run the second cell again. Expect training to take about 10 minutes. For some of the complex environments, you may need to increase the `num_iterations` to 100,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lEBCYVpqU07z",
        "colab": {}
      },
      "source": [
        "# Run this cell before training on a new environment!\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mmoO3zoSGVE",
        "outputId": "7fe3d2c1-a0b5-4c24-d28d-2e7bfd9bb831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "1b7a726cb17f452c839ecf3439d7b3f3",
            "26e2aa4cf8e94385b04fcb1f5130a1ce",
            "6862f74fc7ac42f283ff6a6df786ab37",
            "6537c285fd0443d68c24b0e2313e6ebc",
            "12a75622548441789c89f36f3d5f524b",
            "36b6f872e3884378b9554e5989b53a49",
            "b878cfe1dcdb43afb1922a48691f87c9",
            "25277840064943c89e17f00e5b26e149"
          ]
        }
      },
      "source": [
        "# If you change the environment parameters below, make sure to run\n",
        "# tf.reset_default_graph() in the cell above before training.\n",
        "max_episode_steps = 20\n",
        "env_name = 'FourRooms'  # Choose one of the environments shown above.\n",
        "resize_factor = 5  # Inflate the environment to increase the difficulty.\n",
        "\n",
        "tf_env = env_load_fn(env_name, max_episode_steps,\n",
        "                     resize_factor=resize_factor,\n",
        "                     terminate_on_timeout=False)\n",
        "eval_tf_env = env_load_fn(env_name, max_episode_steps,\n",
        "                          resize_factor=resize_factor,\n",
        "                          terminate_on_timeout=True)\n",
        "agent = UvfAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    max_episode_steps=max_episode_steps,\n",
        "    use_distributional_rl=True,\n",
        "    ensemble_size=3)\n",
        "\n",
        "train_eval(\n",
        "    agent,\n",
        "    tf_env,\n",
        "    eval_tf_env,\n",
        "    initial_collect_steps=1000,\n",
        "    eval_interval=1000,\n",
        "    num_eval_episodes=10,\n",
        "    num_iterations=100,#30000,\n",
        ")\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "random_seed = 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.while_loop(c, b, vars, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b7a726cb17f452c839ecf3439d7b3f3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=2.938631>, extra=(<tf.Tensor: shape=(), dtype=float32, numpy=0.0004577161>, <tf.Tensor: shape=(), dtype=float32, numpy=2.9381733>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N6tOct1DKMyJ",
        "outputId": "ae6d3900-7807-44ed-ee83-a6b775d1ed8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573,
          "referenced_widgets": [
            "01ca2320a0b745f78ddcb12a14ae5f7c",
            "81b71949d5264390b484bb548f8fd7ac",
            "447c38f1ec2946e28774d6f92f5c3369",
            "9441b81dc5a94b39b8600e35392a591a",
            "94a3fd66e84f4b47ba049e0638407647",
            "5026bea971444b3b9b88248aaef39d6f",
            "0ff8f32b07b447e686cb52a313762ca1",
            "28c3654887af45348484dc16016ce9bc"
          ]
        }
      },
      "source": [
        "# Second run of our agent with disabled training\n",
        "env_name2 = 'Maze11x11'  # Choose one of the environments shown above. Different from first one\n",
        "resize_factor2 = 5 # may need to change resize factor if first env too diff in size\n",
        "\n",
        "tf_env2 = env_load_fn(env_name2, max_episode_steps,\n",
        "                     resize_factor=resize_factor2,\n",
        "                     terminate_on_timeout=False)\n",
        "eval_tf_env2 = env_load_fn(env_name2, max_episode_steps,\n",
        "                          resize_factor=resize_factor2,\n",
        "                          terminate_on_timeout=True)\n",
        "# agent.set_trainable_layers(1) \n",
        "# limit trainable layers from above cell, train again.\n",
        "train_eval(\n",
        "    agent,\n",
        "    tf_env2,\n",
        "    eval_tf_env2,\n",
        "    initial_collect_steps=1000,\n",
        "    eval_interval=1000,\n",
        "    num_eval_episodes=10,\n",
        "    num_iterations=30000,\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "random_seed = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01ca2320a0b745f78ddcb12a14ae5f7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:step = 1000, loss = 2.205360\n",
            "INFO:tensorflow:60.900 steps/sec\n",
            "INFO:tensorflow:step = 1000\n",
            "INFO:tensorflow:\t dist = 2\n",
            "INFO:tensorflow:\t\t AverageReturn = -15.30\n",
            "INFO:tensorflow:\t\t predicted_dist = 15.5 (0.7)\n",
            "INFO:tensorflow:\t dist = 5\n",
            "INFO:tensorflow:\t\t AverageReturn = -18.50\n",
            "INFO:tensorflow:\t\t predicted_dist = 16.2 (0.7)\n",
            "INFO:tensorflow:\t dist = 10\n",
            "INFO:tensorflow:\t\t AverageReturn = -19.70\n",
            "INFO:tensorflow:\t\t predicted_dist = 16.2 (0.5)\n",
            "INFO:tensorflow:\t eval_time = 2.35\n",
            "INFO:tensorflow:step = 2000, loss = 1.483013\n",
            "INFO:tensorflow:60.793 steps/sec\n",
            "INFO:tensorflow:step = 2000\n",
            "INFO:tensorflow:\t dist = 2\n",
            "INFO:tensorflow:\t\t AverageReturn = -15.10\n",
            "INFO:tensorflow:\t\t predicted_dist = 14.8 (1.6)\n",
            "INFO:tensorflow:\t dist = 5\n",
            "INFO:tensorflow:\t\t AverageReturn = -20.00\n",
            "INFO:tensorflow:\t\t predicted_dist = 15.7 (2.1)\n",
            "INFO:tensorflow:\t dist = 10\n",
            "INFO:tensorflow:\t\t AverageReturn = -19.90\n",
            "INFO:tensorflow:\t\t predicted_dist = 17.4 (1.3)\n",
            "INFO:tensorflow:\t eval_time = 1.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KxFFTj3Ynv03"
      },
      "source": [
        "-------\n",
        "## Visualization\n",
        "Now, let's visualize some rollouts from the learned policy. Below, you can change the difficulty of the task, which moves the goals closer or further from the starting location. You can change the distance to the goal using the slider below. Notice that, if the goals are nearby, the agent can always reach them. If the goals are far away, the agent rarely reaches them. If only we could lay down a set of \"breadcrumbs\" that led the agent to the goal..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "ZVmiN6xpvNaL",
        "colab": {}
      },
      "source": [
        "#@title Visualize rollouts. {run: \"auto\" }\n",
        "eval_tf_env.pyenv.envs[0]._duration = 100  # We'll give the agent lots of time to try to find the goal.\n",
        "difficulty = 0.6 #@param {min:0, max: 1, step: 0.1, type:\"slider\"}\n",
        "max_goal_dist = eval_tf_env.pyenv.envs[0].gym.max_goal_dist\n",
        "eval_tf_env.pyenv.envs[0].gym.set_sample_goal_args(\n",
        "    prob_constraint=1.0,\n",
        "    min_dist=max(0, max_goal_dist * (difficulty - 0.05)),\n",
        "    max_dist=max_goal_dist * (difficulty + 0.05))\n",
        "\n",
        "\n",
        "def get_rollout(tf_env, policy, seed=None):\n",
        "  np.random.seed(seed)  # Use the same task for both policies.\n",
        "  obs_vec = []\n",
        "  waypoint_vec = []\n",
        "  ts = tf_env.reset()\n",
        "  goal = ts.observation['goal'].numpy()[0]\n",
        "  for _ in tqdm.tnrange(tf_env.pyenv.envs[0]._duration):\n",
        "    obs_vec.append(ts.observation['observation'].numpy()[0])\n",
        "    action = policy.action(ts)\n",
        "    waypoint_vec.append(ts.observation['goal'].numpy()[0])\n",
        "    ts = tf_env.step(action)\n",
        "    if ts.is_last():\n",
        "      break\n",
        "  obs_vec.append(ts.observation['observation'].numpy()[0])\n",
        "  obs_vec = np.array(obs_vec)\n",
        "  waypoint_vec = np.array(waypoint_vec)\n",
        "  return obs_vec, goal, waypoint_vec\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "for col_index in range(2):\n",
        "  plt.subplot(1, 2, col_index + 1)\n",
        "  plot_walls(eval_tf_env.pyenv.envs[0].env.walls)\n",
        "  obs_vec, goal, _ = get_rollout(eval_tf_env, agent.policy)\n",
        "\n",
        "  plt.plot(obs_vec[:, 0], obs_vec[:, 1], 'b-o', alpha=0.3)\n",
        "  plt.scatter([obs_vec[0, 0]], [obs_vec[0, 1]], marker='+',\n",
        "              color='red', s=200, label='start')\n",
        "  plt.scatter([obs_vec[-1, 0]], [obs_vec[-1, 1]], marker='+',\n",
        "              color='green', s=200, label='end')\n",
        "  plt.scatter([goal[0]], [goal[1]], marker='*',\n",
        "              color='green', s=200, label='goal')\n",
        "  if col_index == 0:\n",
        "    plt.legend(loc='lower left', bbox_to_anchor=(0.3, 1), ncol=3, fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "llYSSGRVoIDj"
      },
      "source": [
        "We now will implement the search policy, which automatically finds these waypoints via graph search. The first step is to fill the replay buffer with random data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "6-TCcL5Q9Kn_",
        "colab": {}
      },
      "source": [
        "#@title Fill the replay buffer with random data  {vertical-output: true, run: \"auto\" }\n",
        "replay_buffer_size = 1000 #@param {min:100, max: 1000, step: 100, type:\"slider\"}\n",
        "\n",
        "eval_tf_env.pyenv.envs[0].gym.set_sample_goal_args(\n",
        "    prob_constraint=0.0,\n",
        "    min_dist=0,\n",
        "    max_dist=np.inf)\n",
        "rb_vec = []\n",
        "for _ in tqdm.tnrange(replay_buffer_size):\n",
        "  ts = eval_tf_env.reset()\n",
        "  rb_vec.append(ts.observation['observation'].numpy()[0])\n",
        "rb_vec = np.array(rb_vec)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(*rb_vec.T)\n",
        "plot_walls(eval_tf_env.pyenv.envs[0].env.walls)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yhW1jzfNoR4w"
      },
      "source": [
        "As a sanity check, we'll plot the pairwise distances between all observations in the replay buffer. We expect to see a range of values from 1 to 20. Distributional RL implicitly caps the maximum predicted distance by the largest bin. We've used 20 bins, so the critic predicts 20 for all states that are at least 20 steps away from one another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "gIo_CYsZu6Qy",
        "colab": {}
      },
      "source": [
        "#@title Compute the pairwise distances { vertical-output: true}\n",
        "pdist = agent._get_pairwise_dist(rb_vec, aggregate=None).numpy()\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.hist(pdist.flatten(), bins=range(20))\n",
        "plt.xlabel('predicted distance')\n",
        "plt.ylabel('number of (s, g) pairs')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bWVW45bzozca"
      },
      "source": [
        "With these distances, we can construct a graph. Nodes in the graph are observations in our replay buffer. We connect observations with edges whose lengths are equal to the predicted distance between those observations. Since it is hard to visualize the edge lengths, we included a slider that allows you to only show edges whose predicted length is less than some threshold.\n",
        "\n",
        "Our method learns a collection of critics, each of which makes an independent prediction for the distance between two states. Because each network may make bad predictions for pairs of states it hasn't seen before, we act in a *risk-averse* manner by using the maximum predicted distance across our ensemble. That is, we act pessimistically, only adding an edge if *all* critics think that this pair of states is nearby. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "30X6CutHy_9W",
        "colab": {}
      },
      "source": [
        "#@title Graph Construction { vertical-output: true, run: \"auto\" }\n",
        "cutoff = 13 #@param {min:0, max: 20, type:\"slider\"}\n",
        "# To make visualization easier, we only display the shortest edges for each\n",
        "# node. We will use all edges for planning.\n",
        "edges_to_display = 8\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "plot_walls(eval_tf_env.pyenv.envs[0].env.walls)\n",
        "pdist_combined = np.max(pdist, axis=0)\n",
        "plt.scatter(*rb_vec.T)\n",
        "for i, s_i in enumerate(tqdm.tqdm_notebook(rb_vec)):\n",
        "  for count, j in enumerate(np.argsort(pdist_combined[i])):\n",
        "    if count < edges_to_display and pdist_combined[i, j] < cutoff:\n",
        "      s_j = rb_vec[j]\n",
        "      plt.plot([s_i[0], s_j[0]], [s_i[1], s_j[1]], c='k', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZAfUMydwp16_"
      },
      "source": [
        "We can also visualize the predictions from each critic. Note that while each critic may make incorrect decisions for distant states, their predictions in aggregate are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "x_yH1vFNcuRj",
        "colab": {}
      },
      "source": [
        "#@title Ensemble of Critics { vertical-output: true, run: \"auto\" }\n",
        "cutoff = 5 #@param {min:0, max: 20, type:\"slider\"}\n",
        "edges_to_display = 8\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "for col_index in range(agent._ensemble_size):\n",
        "  plt.subplot(1, agent._ensemble_size, col_index + 1)\n",
        "  plot_walls(eval_tf_env.pyenv.envs[0].env.walls)\n",
        "  plt.title('critic %d' % (col_index + 1))\n",
        "\n",
        "  plt.scatter(*rb_vec.T)\n",
        "  desc='critic %d / %d' % (col_index + 1, agent._ensemble_size)\n",
        "  for i, s_i in enumerate(tqdm.tqdm_notebook(rb_vec, desc=desc)):\n",
        "    for count, j in enumerate(np.argsort(pdist[col_index, i])):\n",
        "      if count < edges_to_display and pdist[col_index, i, j] < cutoff:\n",
        "        s_j = rb_vec[j]\n",
        "        plt.plot([s_i[0], s_j[0]], [s_i[1], s_j[1]], c='k', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NHCpUMJmSDzq"
      },
      "source": [
        "### Search Policy\n",
        "Now, we will combine the goal-conditioned policy and the distance estimates to form a search policy. Internally, this policy performs search over the replay buffer to find a set of waypoints leading to the goal. It then takes actions to reach each waypoint in turn. Because the search happens internally, this policy is a drop-in replacement for any other goal-conditioned policy.\n",
        "\n",
        "We used a *closed loop* policy in our paper, recomputing the search path after each step. Below, we implement both the original closed loop version as well as an *open loop* version, which only performs search once at the start of the episode. The open loop version is much faster, but may perform worse in stochastic environments where replanning is important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "tXFDgreyOodM",
        "colab": {}
      },
      "source": [
        "#@title Implement the search policy\n",
        "class SearchPolicy(tf_policy.Base):\n",
        "\n",
        "  def __init__(self, agent, open_loop=False):\n",
        "    self._agent = agent\n",
        "    self._open_loop = open_loop\n",
        "    self._g = self._build_graph()\n",
        "    super(SearchPolicy, self).__init__(agent.policy.time_step_spec,\n",
        "                                       agent.policy.action_spec)\n",
        "\n",
        "  def _build_graph(self):\n",
        "    g = nx.DiGraph()\n",
        "    pdist_combined = np.max(pdist, axis=0)\n",
        "    for i, s_i in enumerate(rb_vec):\n",
        "      for j, s_j in enumerate(rb_vec):\n",
        "        length = pdist_combined[i, j]\n",
        "        if length < self._agent._max_search_steps:\n",
        "          g.add_edge(i, j, weight=length)\n",
        "    return g\n",
        "\n",
        "  def _get_path(self, time_step):\n",
        "    start_to_rb = agent._get_pairwise_dist(ts.observation['observation'],\n",
        "                                           rb_vec,\n",
        "                                           aggregate='min',\n",
        "                                           masked=True).numpy().flatten()\n",
        "    rb_to_goal = agent._get_pairwise_dist(rb_vec,\n",
        "                                          ts.observation['goal'],\n",
        "                                          aggregate='min',\n",
        "                                          masked=True).numpy().flatten()\n",
        "\n",
        "    g2 = self._g.copy()\n",
        "    for i, (dist_from_start, dist_to_goal) in enumerate(zip(start_to_rb, rb_to_goal)):\n",
        "      if dist_from_start < self._agent._max_search_steps:\n",
        "        g2.add_edge('start', i, weight=dist_from_start)\n",
        "      if dist_to_goal < self._agent._max_search_steps:\n",
        "        g2.add_edge(i, 'goal', weight=dist_to_goal)\n",
        "    path = nx.shortest_path(g2, 'start', 'goal')\n",
        "    edge_lengths = []\n",
        "    for (i, j) in zip(path[:-1], path[1:]):\n",
        "      edge_lengths.append(g2[i][j]['weight'])\n",
        "    wypt_to_goal_dist = np.cumsum(edge_lengths[::-1])[::-1]  # Reverse CumSum\n",
        "    waypoint_vec = list(path)[1:-1]\n",
        "    return waypoint_vec, wypt_to_goal_dist[1:]\n",
        "\n",
        "  def _action(self, time_step, policy_state=(), seed=None):\n",
        "    goal = time_step.observation['goal']\n",
        "    dist_to_goal = self._agent._get_dist_to_goal(time_step)[0].numpy()\n",
        "    if self._open_loop:\n",
        "      if time_step.is_first():\n",
        "        self._waypoint_vec, self._wypt_to_goal_dist_vec = self._get_path(time_step)\n",
        "        self._waypoint_counter = 0\n",
        "      waypoint = rb_vec[self._waypoint_vec[self._waypoint_counter]]\n",
        "      time_step.observation['goal'] = waypoint[None]\n",
        "      dist_to_waypoint = self._agent._get_dist_to_goal(time_step)[0].numpy()\n",
        "      if dist_to_waypoint < self._agent._max_search_steps:\n",
        "        self._waypoint_counter = min(self._waypoint_counter + 1,\n",
        "                                   len(self._waypoint_vec) - 1)\n",
        "        waypoint = rb_vec[self._waypoint_vec[self._waypoint_counter]]\n",
        "        time_step.observation['goal'] = waypoint[None]\n",
        "        dist_to_waypoint = self._agent._get_dist_to_goal(time_step._replace())[0].numpy()\n",
        "      dist_to_goal_via_wypt = dist_to_waypoint + self._wypt_to_goal_dist_vec[self._waypoint_counter]\n",
        "    else:\n",
        "      (waypoint, dist_to_goal_via_wypt) = self._agent._get_waypoint(time_step)\n",
        "      dist_to_goal_via_wypt = dist_to_goal_via_wypt.numpy()\n",
        "\n",
        "    if (dist_to_goal_via_wypt < dist_to_goal) or \\\n",
        "        (dist_to_goal > self._agent._max_search_steps):\n",
        "      time_step.observation['goal'] = tf.convert_to_tensor(value=waypoint[None])\n",
        "    else:\n",
        "      time_step.observation['goal'] = goal\n",
        "    return self._agent.policy.action(time_step, policy_state, seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fc5_wBqF5QnD"
      },
      "source": [
        "Let's initialize the search policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5kWC2q4UujP",
        "colab": {}
      },
      "source": [
        "agent.initialize_search(rb_vec, max_search_steps=7)\n",
        "search_policy = SearchPolicy(agent, open_loop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RO6k_DbOzWig"
      },
      "source": [
        "Now, let's plot the search path found by the search policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "9OvZr9VIRmYA",
        "colab": {}
      },
      "source": [
        "#@title Search Path. { vertical-output: false, run: \"auto\"}\n",
        "\n",
        "difficulty = 0.5 #@param {min:0, max: 1, step: 0.1, type:\"slider\"}\n",
        "max_goal_dist = eval_tf_env.pyenv.envs[0].gym.max_goal_dist\n",
        "eval_tf_env.pyenv.envs[0].gym.set_sample_goal_args(\n",
        "    prob_constraint=1.0,\n",
        "    min_dist=max(0, max_goal_dist * (difficulty - 0.05)),\n",
        "    max_dist=max_goal_dist * (difficulty + 0.05))\n",
        "ts = eval_tf_env.reset()\n",
        "start = ts.observation['observation'].numpy()[0]\n",
        "goal = ts.observation['goal'].numpy()[0]\n",
        "search_policy.action(ts)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plot_walls(eval_tf_env.pyenv.envs[0].env.walls)\n",
        "\n",
        "waypoint_vec = [start]\n",
        "for waypoint_index in search_policy._waypoint_vec:\n",
        "  waypoint_vec.append(rb_vec[waypoint_index])\n",
        "waypoint_vec.append(goal)\n",
        "waypoint_vec = np.array(waypoint_vec)\n",
        "\n",
        "plt.scatter([start[0]], [start[1]], marker='+',\n",
        "            color='red', s=200, label='start')\n",
        "plt.scatter([goal[0]], [goal[1]], marker='*',\n",
        "            color='green', s=200, label='goal')\n",
        "plt.plot(waypoint_vec[:, 0], waypoint_vec[:, 1], 'y-s', alpha=0.3, label='waypoint')\n",
        "plt.legend(loc='lower left', bbox_to_anchor=(-0.1, -0.15), ncol=4, fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UIq3YzOeqYkW"
      },
      "source": [
        "Now, we'll use that path to guide the agent towards the goal. On the left, we plot rollouts from the baseline goal-conditioned policy. On the right, we use that same policy to reach each of the waypoints leading to the goal. As before, the slider allows you to change the distance to the goal. Note that only the search policy is able to reach distant goals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "IK4ukv7TDoWE",
        "colab": {}
      },
      "source": [
        "#@title Rollouts with Search. { vertical-output: true, run: \"auto\"}\n",
        "eval_tf_env.pyenv.envs[0]._duration = 300\n",
        "seed = np.random.randint(0, 1000000)\n",
        "\n",
        "difficulty = 0.8 #@param {min:0, max: 1, step: 0.1, type:\"slider\"}\n",
        "max_goal_dist = eval_tf_env.pyenv.envs[0].gym.max_goal_dist\n",
        "eval_tf_env.pyenv.envs[0].gym.set_sample_goal_args(\n",
        "    prob_constraint=1.0,\n",
        "    min_dist=max(0, max_goal_dist * (difficulty - 0.05)),\n",
        "    max_dist=max_goal_dist * (difficulty + 0.05))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "for col_index in range(2):\n",
        "  title = 'no search' if col_index == 0 else 'search'\n",
        "  plt.subplot(1, 2, col_index + 1)\n",
        "  plot_walls(eval_tf_env.pyenv.envs[0].env.walls)\n",
        "  use_search = (col_index == 1)\n",
        "  np.random.seed(seed)\n",
        "  ts = eval_tf_env.reset()\n",
        "  goal = ts.observation['goal'].numpy()[0]\n",
        "  start = ts.observation['observation'].numpy()[0]\n",
        "  obs_vec = []\n",
        "  for _ in tqdm.tnrange(eval_tf_env.pyenv.envs[0]._duration,\n",
        "                        desc='rollout %d / 2' % (col_index + 1)):\n",
        "    if ts.is_last():\n",
        "      break\n",
        "    obs_vec.append(ts.observation['observation'].numpy()[0])\n",
        "    if use_search:\n",
        "      action = search_policy.action(ts)\n",
        "    else:\n",
        "      action = agent.policy.action(ts)\n",
        "\n",
        "    ts = eval_tf_env.step(action)\n",
        "\n",
        "  obs_vec = np.array(obs_vec)\n",
        "\n",
        "  plt.plot(obs_vec[:, 0], obs_vec[:, 1], 'b-o', alpha=0.3)\n",
        "  plt.scatter([obs_vec[0, 0]], [obs_vec[0, 1]], marker='+',\n",
        "              color='red', s=200, label='start')\n",
        "  plt.scatter([obs_vec[-1, 0]], [obs_vec[-1, 1]], marker='+',\n",
        "              color='green', s=200, label='end')\n",
        "  plt.scatter([goal[0]], [goal[1]], marker='*',\n",
        "              color='green', s=200, label='goal')\n",
        "\n",
        "  plt.title(title, fontsize=24)\n",
        "  if use_search:\n",
        "    waypoint_vec = [start]\n",
        "    for waypoint_index in search_policy._waypoint_vec:\n",
        "      waypoint_vec.append(rb_vec[waypoint_index])\n",
        "    waypoint_vec.append(goal)\n",
        "    waypoint_vec = np.array(waypoint_vec)\n",
        "\n",
        "    plt.plot(waypoint_vec[:, 0], waypoint_vec[:, 1], 'y-s', alpha=0.3, label='waypoint')\n",
        "    plt.legend(loc='lower left', bbox_to_anchor=(-0.8, -0.15), ncol=4, fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2JvMnn7X2mrf"
      },
      "source": [
        "-----------------------------\n",
        "## Next Steps and Open Problems\n",
        "We encourage readers to play around with the experiments. To get started, here are a few questions:\n",
        "1. What effect does distributional RL have on learning distances? (Hint: change `use_distributional_rl` when initializing the `UvfAgent`.)\n",
        "2. What is the effect of using more critic networks in the ensemble? (Hint: change `ensemble_size` when initializing the `UvfAgent`)\n",
        "3. While we applied planning *after* training a goal-conditioned policy, can planning be used to accelerate learning of the goal-conditioned policy? (Hint: Set `UvfAgent.collect_policy` to be the `SearchPolicy`)\n",
        "4. Can you be smart about which observations to include in the replay buffer to make search faster? (Hint: Simple behavior cloning may be enough.)\n",
        "7. Can the search policy be distilled into a single neural network policy?\n",
        "5. What tricks are important for learning distances with RL?\n",
        "6. How can more sophisticated planning algorithms be used in a similar framework?\n",
        "8. Can the same idea by applied to other domains such, as manipulation?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r4FA69AqWJ4t",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0_QyWB6JqORM",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}