{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates a multi-arm bandit problem, and trains a network on it\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from bandit import BanditProblem\n",
    "from learner import SimpleRNN, makeObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assure_equal_and_not_empty(a,b):\n",
    "    return len(a) == len(b) and len(a) > 0\n",
    "\n",
    "\n",
    "def step(gamma, optimizer, records):\n",
    "    \"\"\"implements REINFORCE for learning\n",
    "    \"\"\"\n",
    "    rewards = records.get(\"reward\")\n",
    "    log_probs = records.get(\"log_prob\")\n",
    "    assure_equal_and_not_empty(rewards, log_probs)\n",
    "\n",
    "    R = 0\n",
    "    discount_rewards = []\n",
    "    policy_loss = []\n",
    "\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        discount_rewards.insert(0, R)\n",
    "\n",
    "    for log_prob, reward in zip(log_probs, discount_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "class EpisodeRecorder:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def record(self, thing, x):\n",
    "        if thing in self.data:\n",
    "            self.data[thing].append(x)\n",
    "        else:\n",
    "            self.data[thing] = [x]\n",
    "    \n",
    "    def get(self, thing):\n",
    "        \"\"\"Returns the recorded data\n",
    "        \n",
    "        Note: Raise an exception if `thing` was not recorded..\n",
    "        \"\"\"\n",
    "        return self.data[thing]\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_episodes, sequence_length, gamma, display_epochs):\n",
    "    # create a meta-learner\n",
    "    s = SimpleRNN(hidden_size=20, layers=2)\n",
    "    optimizer = optim.SGD(s.parameters(), lr = 0.01, momentum=0.9)\n",
    "    fantastic = EpisodeRecorder()\n",
    "\n",
    "    K = 1\n",
    "    for i in range(n_episodes // K):\n",
    "\n",
    "        # reset the hidden state after every K environments\n",
    "        s.reset_hidden()\n",
    "        for y in range(K):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # reset the environment\n",
    "            action = 0\n",
    "            reward = 0\n",
    "            epi = EpisodeRecorder()\n",
    "            b = BanditProblem()\n",
    "\n",
    "            for t in range(sequence_length):\n",
    "                done = (t == sequence_length - 1)\n",
    "\n",
    "                # consult our neural network\n",
    "                obs = makeObservation(0, action, reward, done)\n",
    "                probs = s.forward(obs)\n",
    "                epi.record(\"log_prob\", probs)\n",
    "\n",
    "                # choose the max action\n",
    "                # (this is the part that isn't differentiable!)\n",
    "                di = Categorical(probs)\n",
    "                action = di.sample()\n",
    "                epi.record(\"action\", action)\n",
    "\n",
    "                # update the last reward\n",
    "                reward = b.pull(action)\n",
    "                epi.record(\"reward\", reward)\n",
    "\n",
    "            # upgrade our neural network\n",
    "            step(gamma, optimizer, epi)\n",
    "\n",
    "            # before we destroy the environment\n",
    "            fantastic.record(\"action_mean\", np.mean(epi.get(\"action\")))\n",
    "            fantastic.record(\"action_var\", np.std(epi.get(\"action\")))\n",
    "            fantastic.record(\"average_reward\", np.mean(epi.get(\"reward\")))\n",
    "            fantastic.record(\"time\", time.time() - start_time)\n",
    "\n",
    "            # is it learning?\n",
    "            current_epoch = K*i +y\n",
    "            if current_epoch % display_epochs == 0:\n",
    "                display = \"\"\n",
    "                display += \"Episode {}, Time (elapsed {:.0f}, {:.4f}s/episode), \".format(current_epoch, np.sum(fantastic.get(\"time\")), np.mean(fantastic.get(\"time\")[-display_epochs:]))\n",
    "                display += \"Reward (avg {:.4f}) \".format(np.mean(fantastic.get(\"average_reward\")[-display_epochs:]))\n",
    "                display += \"Actions (std.dev {:.4f}) \".format(fantastic.get(\"action_var\")[-1])\n",
    "                print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Time (elapsed 0, 0.0169s/episode), Reward (avg 0.2000) Actions (std.dev 0.4899) \n",
      "Episode 1, Time (elapsed 0, 0.0150s/episode), Reward (avg 1.0000) Actions (std.dev 0.4000) \n",
      "Episode 2, Time (elapsed 0, 0.0185s/episode), Reward (avg 0.2000) Actions (std.dev 0.4000) \n",
      "Episode 3, Time (elapsed 0, 0.0137s/episode), Reward (avg 0.8000) Actions (std.dev 0.4899) \n",
      "Episode 4, Time (elapsed 0, 0.0166s/episode), Reward (avg 0.0000) Actions (std.dev 0.0000) \n",
      "Episode 5, Time (elapsed 0, 0.0224s/episode), Reward (avg 0.6000) Actions (std.dev 0.4899) \n",
      "Episode 6, Time (elapsed 0, 0.0176s/episode), Reward (avg 0.2000) Actions (std.dev 0.4899) \n",
      "Episode 7, Time (elapsed 0, 0.0159s/episode), Reward (avg 0.4000) Actions (std.dev 0.4899) \n",
      "Episode 8, Time (elapsed 0, 0.0190s/episode), Reward (avg 1.0000) Actions (std.dev 0.4899) \n",
      "Episode 9, Time (elapsed 0, 0.0176s/episode), Reward (avg 0.8000) Actions (std.dev 0.4000) \n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10\n",
    "sequence_length = 5\n",
    "display_epochs = 1\n",
    "gamma = 0.5\n",
    "\n",
    "run(n_episodes, sequence_length, gamma, display_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
